{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769bd6c5-c710-46e7-944a-3bcda3364c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llama.cpp\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
    "!pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b1bca9-8e7e-4143-8fd0-4ca155bd3fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\n",
      "Cloning into 'Mixtral-8x7B-Instruct-v0.1'...\n",
      "remote: Enumerating objects: 63, done.\u001b[K\n",
      "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
      "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
      "remote: Total 63 (delta 16), reused 0 (delta 0), pack-reused 1\u001b[K\n",
      "Unpacking objects: 100% (63/63), 473.58 KiB | 7.64 MiB/s, done.\n",
      "^C\n",
      "\n",
      "Exiting because of \"interrupt\" signal.\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "MODEL_ID = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
    "\n",
    "# Download model\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/{MODEL_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33b626a4-d6af-437b-9ee7-db325286f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00001-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00001-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00002-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00003-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00004-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00005-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00006-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00007-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00008-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00009-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00010-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00011-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00012-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00013-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00014-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00015-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00016-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00017-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00018-of-00019.safetensors\n",
      "Loading model file ../Mixtral-8x7B-Instruct-v0.1/model-00019-of-00019.safetensors\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, n_experts=8, n_experts_used=2, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=1000000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('../Mixtral-8x7B-Instruct-v0.1'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 58980 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [32000, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.0.w1.weight -> blk.0.ffn_gate.0.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.0.w2.weight -> blk.0.ffn_down.0.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.0.block_sparse_moe.experts.0.w3.weight -> blk.0.ffn_up.0.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.1.w1.weight -> blk.0.ffn_gate.1.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.1.w2.weight -> blk.0.ffn_down.1.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.0.block_sparse_moe.experts.1.w3.weight -> blk.0.ffn_up.1.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.2.w1.weight -> blk.0.ffn_gate.2.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.2.w2.weight -> blk.0.ffn_down.2.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.0.block_sparse_moe.experts.2.w3.weight -> blk.0.ffn_up.2.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.3.w1.weight -> blk.0.ffn_gate.3.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.3.w2.weight -> blk.0.ffn_down.3.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.0.block_sparse_moe.experts.3.w3.weight -> blk.0.ffn_up.3.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.4.w1.weight -> blk.0.ffn_gate.4.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.4.w2.weight -> blk.0.ffn_down.4.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.0.block_sparse_moe.experts.4.w3.weight -> blk.0.ffn_up.4.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.5.w1.weight -> blk.0.ffn_gate.5.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.5.w2.weight -> blk.0.ffn_down.5.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.0.block_sparse_moe.experts.5.w3.weight -> blk.0.ffn_up.5.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.6.w1.weight -> blk.0.ffn_gate.6.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.6.w2.weight -> blk.0.ffn_down.6.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.0.block_sparse_moe.experts.6.w3.weight -> blk.0.ffn_up.6.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.7.w1.weight -> blk.0.ffn_gate.7.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.experts.7.w2.weight -> blk.0.ffn_down.7.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.0.block_sparse_moe.experts.7.w3.weight -> blk.0.ffn_up.7.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.0.block_sparse_moe.gate.weight      -> blk.0.ffn_gate_inp.weight                | BF16   | [8, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.0.w1.weight -> blk.1.ffn_gate.0.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.0.w2.weight -> blk.1.ffn_down.0.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.1.block_sparse_moe.experts.0.w3.weight -> blk.1.ffn_up.0.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.1.w1.weight -> blk.1.ffn_gate.1.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.1.w2.weight -> blk.1.ffn_down.1.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.1.block_sparse_moe.experts.1.w3.weight -> blk.1.ffn_up.1.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.2.w1.weight -> blk.1.ffn_gate.2.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.2.w2.weight -> blk.1.ffn_down.2.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.1.block_sparse_moe.experts.2.w3.weight -> blk.1.ffn_up.2.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.3.w1.weight -> blk.1.ffn_gate.3.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.3.w2.weight -> blk.1.ffn_down.3.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.1.block_sparse_moe.experts.3.w3.weight -> blk.1.ffn_up.3.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.4.w1.weight -> blk.1.ffn_gate.4.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.4.w2.weight -> blk.1.ffn_down.4.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.1.block_sparse_moe.gate.weight      -> blk.1.ffn_gate_inp.weight                | BF16   | [8, 4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.4.w3.weight -> blk.1.ffn_up.4.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.5.w1.weight -> blk.1.ffn_gate.5.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.5.w2.weight -> blk.1.ffn_down.5.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.1.block_sparse_moe.experts.5.w3.weight -> blk.1.ffn_up.5.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.6.w1.weight -> blk.1.ffn_gate.6.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.6.w2.weight -> blk.1.ffn_down.6.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.1.block_sparse_moe.experts.6.w3.weight -> blk.1.ffn_up.6.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.7.w1.weight -> blk.1.ffn_gate.7.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.1.block_sparse_moe.experts.7.w2.weight -> blk.1.ffn_down.7.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.1.block_sparse_moe.experts.7.w3.weight -> blk.1.ffn_up.7.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.2.block_sparse_moe.experts.0.w1.weight -> blk.2.ffn_gate.0.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.0.w2.weight -> blk.2.ffn_down.0.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.2.block_sparse_moe.experts.0.w3.weight -> blk.2.ffn_up.0.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.1.w1.weight -> blk.2.ffn_gate.1.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.1.w2.weight -> blk.2.ffn_down.1.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.2.block_sparse_moe.experts.1.w3.weight -> blk.2.ffn_up.1.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.2.w1.weight -> blk.2.ffn_gate.2.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.2.w2.weight -> blk.2.ffn_down.2.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.2.block_sparse_moe.experts.2.w3.weight -> blk.2.ffn_up.2.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.3.w1.weight -> blk.2.ffn_gate.3.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.3.w2.weight -> blk.2.ffn_down.3.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.2.block_sparse_moe.experts.3.w3.weight -> blk.2.ffn_up.3.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.4.w1.weight -> blk.2.ffn_gate.4.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.4.w2.weight -> blk.2.ffn_down.4.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.2.block_sparse_moe.experts.4.w3.weight -> blk.2.ffn_up.4.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.5.w1.weight -> blk.2.ffn_gate.5.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.5.w2.weight -> blk.2.ffn_down.5.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.2.block_sparse_moe.experts.5.w3.weight -> blk.2.ffn_up.5.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.6.w1.weight -> blk.2.ffn_gate.6.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.6.w2.weight -> blk.2.ffn_down.6.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.2.block_sparse_moe.experts.6.w3.weight -> blk.2.ffn_up.6.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.7.w1.weight -> blk.2.ffn_gate.7.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.experts.7.w2.weight -> blk.2.ffn_down.7.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.2.block_sparse_moe.experts.7.w3.weight -> blk.2.ffn_up.7.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.2.block_sparse_moe.gate.weight      -> blk.2.ffn_gate_inp.weight                | BF16   | [8, 4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.0.w1.weight -> blk.3.ffn_gate.0.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.0.w2.weight -> blk.3.ffn_down.0.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.3.block_sparse_moe.experts.0.w3.weight -> blk.3.ffn_up.0.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.1.w1.weight -> blk.3.ffn_gate.1.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.1.w2.weight -> blk.3.ffn_down.1.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.3.block_sparse_moe.experts.1.w3.weight -> blk.3.ffn_up.1.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.2.w1.weight -> blk.3.ffn_gate.2.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.gate.weight      -> blk.3.ffn_gate_inp.weight                | BF16   | [8, 4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.2.w2.weight -> blk.3.ffn_down.2.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.3.block_sparse_moe.experts.2.w3.weight -> blk.3.ffn_up.2.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.3.w1.weight -> blk.3.ffn_gate.3.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.3.w2.weight -> blk.3.ffn_down.3.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.3.block_sparse_moe.experts.3.w3.weight -> blk.3.ffn_up.3.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.4.w1.weight -> blk.3.ffn_gate.4.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.4.w2.weight -> blk.3.ffn_down.4.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.3.block_sparse_moe.experts.4.w3.weight -> blk.3.ffn_up.4.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.5.w1.weight -> blk.3.ffn_gate.5.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.5.w2.weight -> blk.3.ffn_down.5.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.3.block_sparse_moe.experts.5.w3.weight -> blk.3.ffn_up.5.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.6.w1.weight -> blk.3.ffn_gate.6.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.6.w2.weight -> blk.3.ffn_down.6.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.3.block_sparse_moe.experts.6.w3.weight -> blk.3.ffn_up.6.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.7.w1.weight -> blk.3.ffn_gate.7.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.3.block_sparse_moe.experts.7.w2.weight -> blk.3.ffn_down.7.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.3.block_sparse_moe.experts.7.w3.weight -> blk.3.ffn_up.7.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.4.block_sparse_moe.experts.0.w1.weight -> blk.4.ffn_gate.0.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.0.w2.weight -> blk.4.ffn_down.0.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.4.block_sparse_moe.experts.0.w3.weight -> blk.4.ffn_up.0.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.1.w1.weight -> blk.4.ffn_gate.1.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.1.w2.weight -> blk.4.ffn_down.1.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.4.block_sparse_moe.experts.1.w3.weight -> blk.4.ffn_up.1.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.2.w1.weight -> blk.4.ffn_gate.2.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.2.w2.weight -> blk.4.ffn_down.2.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.4.block_sparse_moe.experts.2.w3.weight -> blk.4.ffn_up.2.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.3.w1.weight -> blk.4.ffn_gate.3.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.3.w2.weight -> blk.4.ffn_down.3.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.4.block_sparse_moe.experts.3.w3.weight -> blk.4.ffn_up.3.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.4.w1.weight -> blk.4.ffn_gate.4.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.4.w2.weight -> blk.4.ffn_down.4.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.4.block_sparse_moe.experts.4.w3.weight -> blk.4.ffn_up.4.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.5.w1.weight -> blk.4.ffn_gate.5.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.5.w2.weight -> blk.4.ffn_down.5.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.4.block_sparse_moe.experts.5.w3.weight -> blk.4.ffn_up.5.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.6.w1.weight -> blk.4.ffn_gate.6.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.6.w2.weight -> blk.4.ffn_down.6.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.4.block_sparse_moe.experts.6.w3.weight -> blk.4.ffn_up.6.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.7.w1.weight -> blk.4.ffn_gate.7.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.experts.7.w2.weight -> blk.4.ffn_down.7.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.4.block_sparse_moe.experts.7.w3.weight -> blk.4.ffn_up.7.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.4.block_sparse_moe.gate.weight      -> blk.4.ffn_gate_inp.weight                | BF16   | [8, 4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.5.block_sparse_moe.gate.weight      -> blk.5.ffn_gate_inp.weight                | BF16   | [8, 4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.0.w1.weight -> blk.5.ffn_gate.0.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.0.w2.weight -> blk.5.ffn_down.0.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.5.block_sparse_moe.experts.0.w3.weight -> blk.5.ffn_up.0.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.1.w1.weight -> blk.5.ffn_gate.1.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.1.w2.weight -> blk.5.ffn_down.1.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.5.block_sparse_moe.experts.1.w3.weight -> blk.5.ffn_up.1.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.2.w1.weight -> blk.5.ffn_gate.2.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.2.w2.weight -> blk.5.ffn_down.2.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.5.block_sparse_moe.experts.2.w3.weight -> blk.5.ffn_up.2.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.3.w1.weight -> blk.5.ffn_gate.3.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.3.w2.weight -> blk.5.ffn_down.3.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.5.block_sparse_moe.experts.3.w3.weight -> blk.5.ffn_up.3.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.4.w1.weight -> blk.5.ffn_gate.4.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.4.w2.weight -> blk.5.ffn_down.4.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.5.block_sparse_moe.experts.4.w3.weight -> blk.5.ffn_up.4.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.5.w1.weight -> blk.5.ffn_gate.5.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.5.w2.weight -> blk.5.ffn_down.5.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.5.block_sparse_moe.experts.5.w3.weight -> blk.5.ffn_up.5.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.6.w1.weight -> blk.5.ffn_gate.6.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.6.w2.weight -> blk.5.ffn_down.6.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.5.block_sparse_moe.experts.6.w3.weight -> blk.5.ffn_up.6.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.7.w1.weight -> blk.5.ffn_gate.7.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.5.block_sparse_moe.experts.7.w2.weight -> blk.5.ffn_down.7.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.5.block_sparse_moe.experts.7.w3.weight -> blk.5.ffn_up.7.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.6.block_sparse_moe.experts.0.w1.weight -> blk.6.ffn_gate.0.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.0.w2.weight -> blk.6.ffn_down.0.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.6.block_sparse_moe.experts.0.w3.weight -> blk.6.ffn_up.0.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.1.w1.weight -> blk.6.ffn_gate.1.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.1.w2.weight -> blk.6.ffn_down.1.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.6.block_sparse_moe.experts.1.w3.weight -> blk.6.ffn_up.1.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.2.w1.weight -> blk.6.ffn_gate.2.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.2.w2.weight -> blk.6.ffn_down.2.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.6.block_sparse_moe.experts.2.w3.weight -> blk.6.ffn_up.2.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.3.w1.weight -> blk.6.ffn_gate.3.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.3.w2.weight -> blk.6.ffn_down.3.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.6.block_sparse_moe.experts.3.w3.weight -> blk.6.ffn_up.3.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.4.w1.weight -> blk.6.ffn_gate.4.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.4.w2.weight -> blk.6.ffn_down.4.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.6.block_sparse_moe.experts.4.w3.weight -> blk.6.ffn_up.4.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.5.w1.weight -> blk.6.ffn_gate.5.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.5.w2.weight -> blk.6.ffn_down.5.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.6.block_sparse_moe.gate.weight      -> blk.6.ffn_gate_inp.weight                | BF16   | [8, 4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.5.w3.weight -> blk.6.ffn_up.5.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.6.w1.weight -> blk.6.ffn_gate.6.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.6.w2.weight -> blk.6.ffn_down.6.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.6.block_sparse_moe.experts.6.w3.weight -> blk.6.ffn_up.6.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.7.w1.weight -> blk.6.ffn_gate.7.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.6.block_sparse_moe.experts.7.w2.weight -> blk.6.ffn_down.7.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.6.block_sparse_moe.experts.7.w3.weight -> blk.6.ffn_up.7.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.7.block_sparse_moe.experts.0.w1.weight -> blk.7.ffn_gate.0.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.0.w2.weight -> blk.7.ffn_down.0.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.7.block_sparse_moe.experts.0.w3.weight -> blk.7.ffn_up.0.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.1.w1.weight -> blk.7.ffn_gate.1.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.1.w2.weight -> blk.7.ffn_down.1.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.7.block_sparse_moe.experts.1.w3.weight -> blk.7.ffn_up.1.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.2.w1.weight -> blk.7.ffn_gate.2.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.2.w2.weight -> blk.7.ffn_down.2.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.7.block_sparse_moe.experts.2.w3.weight -> blk.7.ffn_up.2.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.3.w1.weight -> blk.7.ffn_gate.3.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.3.w2.weight -> blk.7.ffn_down.3.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.7.block_sparse_moe.experts.3.w3.weight -> blk.7.ffn_up.3.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.4.w1.weight -> blk.7.ffn_gate.4.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.4.w2.weight -> blk.7.ffn_down.4.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.7.block_sparse_moe.experts.4.w3.weight -> blk.7.ffn_up.4.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.5.w1.weight -> blk.7.ffn_gate.5.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.5.w2.weight -> blk.7.ffn_down.5.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.7.block_sparse_moe.experts.5.w3.weight -> blk.7.ffn_up.5.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.6.w1.weight -> blk.7.ffn_gate.6.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.6.w2.weight -> blk.7.ffn_down.6.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.7.block_sparse_moe.experts.6.w3.weight -> blk.7.ffn_up.6.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.7.w1.weight -> blk.7.ffn_gate.7.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.experts.7.w2.weight -> blk.7.ffn_down.7.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.7.block_sparse_moe.experts.7.w3.weight -> blk.7.ffn_up.7.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.7.block_sparse_moe.gate.weight      -> blk.7.ffn_gate_inp.weight                | BF16   | [8, 4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.0.w1.weight -> blk.8.ffn_gate.0.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.0.w2.weight -> blk.8.ffn_down.0.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.8.block_sparse_moe.experts.0.w3.weight -> blk.8.ffn_up.0.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.1.w1.weight -> blk.8.ffn_gate.1.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.1.w2.weight -> blk.8.ffn_down.1.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.8.block_sparse_moe.experts.1.w3.weight -> blk.8.ffn_up.1.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.2.w1.weight -> blk.8.ffn_gate.2.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.2.w2.weight -> blk.8.ffn_down.2.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.8.block_sparse_moe.experts.2.w3.weight -> blk.8.ffn_up.2.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.3.w1.weight -> blk.8.ffn_gate.3.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.gate.weight      -> blk.8.ffn_gate_inp.weight                | BF16   | [8, 4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.0.w1.weight -> blk.10.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.0.w2.weight -> blk.10.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.10.block_sparse_moe.experts.0.w3.weight -> blk.10.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.gate.weight     -> blk.10.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.3.w2.weight -> blk.8.ffn_down.3.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.8.block_sparse_moe.experts.3.w3.weight -> blk.8.ffn_up.3.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.4.w1.weight -> blk.8.ffn_gate.4.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.4.w2.weight -> blk.8.ffn_down.4.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.8.block_sparse_moe.experts.4.w3.weight -> blk.8.ffn_up.4.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.5.w1.weight -> blk.8.ffn_gate.5.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.5.w2.weight -> blk.8.ffn_down.5.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.8.block_sparse_moe.experts.5.w3.weight -> blk.8.ffn_up.5.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.6.w1.weight -> blk.8.ffn_gate.6.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.6.w2.weight -> blk.8.ffn_down.6.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.8.block_sparse_moe.experts.6.w3.weight -> blk.8.ffn_up.6.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.7.w1.weight -> blk.8.ffn_gate.7.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.8.block_sparse_moe.experts.7.w2.weight -> blk.8.ffn_down.7.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.8.block_sparse_moe.experts.7.w3.weight -> blk.8.ffn_up.7.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.9.block_sparse_moe.experts.0.w1.weight -> blk.9.ffn_gate.0.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.0.w2.weight -> blk.9.ffn_down.0.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.9.block_sparse_moe.experts.0.w3.weight -> blk.9.ffn_up.0.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.1.w1.weight -> blk.9.ffn_gate.1.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.1.w2.weight -> blk.9.ffn_down.1.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.9.block_sparse_moe.experts.1.w3.weight -> blk.9.ffn_up.1.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.2.w1.weight -> blk.9.ffn_gate.2.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.2.w2.weight -> blk.9.ffn_down.2.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.9.block_sparse_moe.experts.2.w3.weight -> blk.9.ffn_up.2.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.3.w1.weight -> blk.9.ffn_gate.3.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.3.w2.weight -> blk.9.ffn_down.3.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.9.block_sparse_moe.experts.3.w3.weight -> blk.9.ffn_up.3.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.4.w1.weight -> blk.9.ffn_gate.4.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.4.w2.weight -> blk.9.ffn_down.4.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.9.block_sparse_moe.experts.4.w3.weight -> blk.9.ffn_up.4.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.5.w1.weight -> blk.9.ffn_gate.5.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.5.w2.weight -> blk.9.ffn_down.5.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.9.block_sparse_moe.experts.5.w3.weight -> blk.9.ffn_up.5.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.6.w1.weight -> blk.9.ffn_gate.6.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.6.w2.weight -> blk.9.ffn_down.6.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.9.block_sparse_moe.experts.6.w3.weight -> blk.9.ffn_up.6.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.7.w1.weight -> blk.9.ffn_gate.7.weight                  | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.experts.7.w2.weight -> blk.9.ffn_down.7.weight                  | BF16   | [4096, 14336]\n",
      "model.layers.9.block_sparse_moe.experts.7.w3.weight -> blk.9.ffn_up.7.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.9.block_sparse_moe.gate.weight      -> blk.9.ffn_gate_inp.weight                | BF16   | [8, 4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.1.w1.weight -> blk.10.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.1.w2.weight -> blk.10.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.10.block_sparse_moe.experts.1.w3.weight -> blk.10.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.2.w1.weight -> blk.10.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.2.w2.weight -> blk.10.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.10.block_sparse_moe.experts.2.w3.weight -> blk.10.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.3.w1.weight -> blk.10.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.3.w2.weight -> blk.10.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.10.block_sparse_moe.experts.3.w3.weight -> blk.10.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.4.w1.weight -> blk.10.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.4.w2.weight -> blk.10.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.10.block_sparse_moe.experts.4.w3.weight -> blk.10.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.5.w1.weight -> blk.10.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.5.w2.weight -> blk.10.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.10.block_sparse_moe.experts.5.w3.weight -> blk.10.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.6.w1.weight -> blk.10.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.6.w2.weight -> blk.10.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.10.block_sparse_moe.experts.6.w3.weight -> blk.10.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.7.w1.weight -> blk.10.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.10.block_sparse_moe.experts.7.w2.weight -> blk.10.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.10.block_sparse_moe.experts.7.w3.weight -> blk.10.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.11.block_sparse_moe.experts.0.w1.weight -> blk.11.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.0.w2.weight -> blk.11.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.11.block_sparse_moe.experts.0.w3.weight -> blk.11.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.1.w1.weight -> blk.11.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.1.w2.weight -> blk.11.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.11.block_sparse_moe.experts.1.w3.weight -> blk.11.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.2.w1.weight -> blk.11.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.2.w2.weight -> blk.11.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.11.block_sparse_moe.experts.2.w3.weight -> blk.11.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.3.w1.weight -> blk.11.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.3.w2.weight -> blk.11.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.11.block_sparse_moe.experts.3.w3.weight -> blk.11.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.4.w1.weight -> blk.11.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.4.w2.weight -> blk.11.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.11.block_sparse_moe.experts.4.w3.weight -> blk.11.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.5.w1.weight -> blk.11.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.5.w2.weight -> blk.11.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.11.block_sparse_moe.experts.5.w3.weight -> blk.11.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.6.w1.weight -> blk.11.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.6.w2.weight -> blk.11.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.11.block_sparse_moe.gate.weight     -> blk.11.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.6.w3.weight -> blk.11.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.7.w1.weight -> blk.11.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.11.block_sparse_moe.experts.7.w2.weight -> blk.11.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.11.block_sparse_moe.experts.7.w3.weight -> blk.11.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.12.block_sparse_moe.experts.0.w1.weight -> blk.12.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.0.w2.weight -> blk.12.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.12.block_sparse_moe.experts.0.w3.weight -> blk.12.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.1.w1.weight -> blk.12.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.1.w2.weight -> blk.12.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.12.block_sparse_moe.experts.1.w3.weight -> blk.12.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.2.w1.weight -> blk.12.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.2.w2.weight -> blk.12.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.12.block_sparse_moe.experts.2.w3.weight -> blk.12.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.3.w1.weight -> blk.12.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.3.w2.weight -> blk.12.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.12.block_sparse_moe.experts.3.w3.weight -> blk.12.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.4.w1.weight -> blk.12.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.4.w2.weight -> blk.12.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.12.block_sparse_moe.experts.4.w3.weight -> blk.12.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.5.w1.weight -> blk.12.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.5.w2.weight -> blk.12.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.12.block_sparse_moe.experts.5.w3.weight -> blk.12.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.6.w1.weight -> blk.12.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.6.w2.weight -> blk.12.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.12.block_sparse_moe.experts.6.w3.weight -> blk.12.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.7.w1.weight -> blk.12.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.experts.7.w2.weight -> blk.12.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.12.block_sparse_moe.experts.7.w3.weight -> blk.12.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.12.block_sparse_moe.gate.weight     -> blk.12.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.0.w1.weight -> blk.13.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.0.w2.weight -> blk.13.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.13.block_sparse_moe.experts.0.w3.weight -> blk.13.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.1.w1.weight -> blk.13.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.1.w2.weight -> blk.13.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.13.block_sparse_moe.experts.1.w3.weight -> blk.13.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.2.w1.weight -> blk.13.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.2.w2.weight -> blk.13.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.13.block_sparse_moe.experts.2.w3.weight -> blk.13.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.3.w1.weight -> blk.13.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.3.w2.weight -> blk.13.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.13.block_sparse_moe.experts.3.w3.weight -> blk.13.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.4.w1.weight -> blk.13.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.gate.weight     -> blk.13.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.4.w2.weight -> blk.13.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.13.block_sparse_moe.experts.4.w3.weight -> blk.13.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.5.w1.weight -> blk.13.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.5.w2.weight -> blk.13.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.13.block_sparse_moe.experts.5.w3.weight -> blk.13.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.6.w1.weight -> blk.13.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.6.w2.weight -> blk.13.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.13.block_sparse_moe.experts.6.w3.weight -> blk.13.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.7.w1.weight -> blk.13.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.13.block_sparse_moe.experts.7.w2.weight -> blk.13.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.13.block_sparse_moe.experts.7.w3.weight -> blk.13.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.14.block_sparse_moe.experts.0.w1.weight -> blk.14.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.0.w2.weight -> blk.14.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.14.block_sparse_moe.experts.0.w3.weight -> blk.14.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.1.w1.weight -> blk.14.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.1.w2.weight -> blk.14.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.14.block_sparse_moe.experts.1.w3.weight -> blk.14.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.2.w1.weight -> blk.14.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.2.w2.weight -> blk.14.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.14.block_sparse_moe.experts.2.w3.weight -> blk.14.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.3.w1.weight -> blk.14.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.3.w2.weight -> blk.14.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.14.block_sparse_moe.experts.3.w3.weight -> blk.14.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.4.w1.weight -> blk.14.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.4.w2.weight -> blk.14.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.14.block_sparse_moe.experts.4.w3.weight -> blk.14.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.5.w1.weight -> blk.14.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.5.w2.weight -> blk.14.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.14.block_sparse_moe.experts.5.w3.weight -> blk.14.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.6.w1.weight -> blk.14.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.6.w2.weight -> blk.14.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.14.block_sparse_moe.experts.6.w3.weight -> blk.14.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.7.w1.weight -> blk.14.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.experts.7.w2.weight -> blk.14.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.14.block_sparse_moe.experts.7.w3.weight -> blk.14.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.14.block_sparse_moe.gate.weight     -> blk.14.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.0.w1.weight -> blk.15.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.0.w2.weight -> blk.15.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.15.block_sparse_moe.experts.0.w3.weight -> blk.15.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.1.w1.weight -> blk.15.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.1.w2.weight -> blk.15.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.15.block_sparse_moe.experts.1.w3.weight -> blk.15.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.gate.weight     -> blk.15.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.2.w1.weight -> blk.15.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.2.w2.weight -> blk.15.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.15.block_sparse_moe.experts.2.w3.weight -> blk.15.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.3.w1.weight -> blk.15.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.3.w2.weight -> blk.15.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.15.block_sparse_moe.experts.3.w3.weight -> blk.15.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.4.w1.weight -> blk.15.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.4.w2.weight -> blk.15.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.15.block_sparse_moe.experts.4.w3.weight -> blk.15.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.5.w1.weight -> blk.15.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.5.w2.weight -> blk.15.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.15.block_sparse_moe.experts.5.w3.weight -> blk.15.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.6.w1.weight -> blk.15.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.6.w2.weight -> blk.15.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.15.block_sparse_moe.experts.6.w3.weight -> blk.15.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.7.w1.weight -> blk.15.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.15.block_sparse_moe.experts.7.w2.weight -> blk.15.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.15.block_sparse_moe.experts.7.w3.weight -> blk.15.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.16.block_sparse_moe.experts.0.w1.weight -> blk.16.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.0.w2.weight -> blk.16.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.16.block_sparse_moe.experts.0.w3.weight -> blk.16.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.1.w1.weight -> blk.16.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.1.w2.weight -> blk.16.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.16.block_sparse_moe.experts.1.w3.weight -> blk.16.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.2.w1.weight -> blk.16.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.2.w2.weight -> blk.16.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.16.block_sparse_moe.experts.2.w3.weight -> blk.16.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.3.w1.weight -> blk.16.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.3.w2.weight -> blk.16.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.16.block_sparse_moe.experts.3.w3.weight -> blk.16.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.4.w1.weight -> blk.16.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.4.w2.weight -> blk.16.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.16.block_sparse_moe.experts.4.w3.weight -> blk.16.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.5.w1.weight -> blk.16.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.5.w2.weight -> blk.16.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.16.block_sparse_moe.experts.5.w3.weight -> blk.16.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.6.w1.weight -> blk.16.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.6.w2.weight -> blk.16.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.16.block_sparse_moe.experts.6.w3.weight -> blk.16.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.7.w1.weight -> blk.16.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.7.w2.weight -> blk.16.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.16.block_sparse_moe.gate.weight     -> blk.16.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.16.block_sparse_moe.experts.7.w3.weight -> blk.16.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.17.block_sparse_moe.experts.0.w1.weight -> blk.17.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.0.w2.weight -> blk.17.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.17.block_sparse_moe.experts.0.w3.weight -> blk.17.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.1.w1.weight -> blk.17.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.1.w2.weight -> blk.17.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.17.block_sparse_moe.experts.1.w3.weight -> blk.17.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.2.w1.weight -> blk.17.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.2.w2.weight -> blk.17.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.17.block_sparse_moe.experts.2.w3.weight -> blk.17.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.3.w1.weight -> blk.17.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.3.w2.weight -> blk.17.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.17.block_sparse_moe.experts.3.w3.weight -> blk.17.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.4.w1.weight -> blk.17.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.4.w2.weight -> blk.17.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.17.block_sparse_moe.experts.4.w3.weight -> blk.17.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.5.w1.weight -> blk.17.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.5.w2.weight -> blk.17.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.17.block_sparse_moe.experts.5.w3.weight -> blk.17.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.6.w1.weight -> blk.17.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.6.w2.weight -> blk.17.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.17.block_sparse_moe.experts.6.w3.weight -> blk.17.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.7.w1.weight -> blk.17.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.experts.7.w2.weight -> blk.17.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.17.block_sparse_moe.experts.7.w3.weight -> blk.17.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.17.block_sparse_moe.gate.weight     -> blk.17.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.0.w1.weight -> blk.18.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.0.w2.weight -> blk.18.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.18.block_sparse_moe.experts.0.w3.weight -> blk.18.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.1.w1.weight -> blk.18.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.1.w2.weight -> blk.18.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.18.block_sparse_moe.experts.1.w3.weight -> blk.18.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.2.w1.weight -> blk.18.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.2.w2.weight -> blk.18.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.18.block_sparse_moe.experts.2.w3.weight -> blk.18.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.3.w1.weight -> blk.18.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.3.w2.weight -> blk.18.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.18.block_sparse_moe.experts.3.w3.weight -> blk.18.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.4.w1.weight -> blk.18.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.4.w2.weight -> blk.18.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.18.block_sparse_moe.experts.4.w3.weight -> blk.18.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.5.w1.weight -> blk.18.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.gate.weight     -> blk.18.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.5.w2.weight -> blk.18.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.18.block_sparse_moe.experts.5.w3.weight -> blk.18.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.6.w1.weight -> blk.18.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.6.w2.weight -> blk.18.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.18.block_sparse_moe.experts.6.w3.weight -> blk.18.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.7.w1.weight -> blk.18.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.18.block_sparse_moe.experts.7.w2.weight -> blk.18.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.18.block_sparse_moe.experts.7.w3.weight -> blk.18.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.19.block_sparse_moe.experts.0.w1.weight -> blk.19.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.0.w2.weight -> blk.19.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.19.block_sparse_moe.experts.0.w3.weight -> blk.19.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.1.w1.weight -> blk.19.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.1.w2.weight -> blk.19.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.19.block_sparse_moe.experts.1.w3.weight -> blk.19.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.2.w1.weight -> blk.19.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.2.w2.weight -> blk.19.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.19.block_sparse_moe.experts.2.w3.weight -> blk.19.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.3.w1.weight -> blk.19.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.3.w2.weight -> blk.19.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.19.block_sparse_moe.experts.3.w3.weight -> blk.19.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.4.w1.weight -> blk.19.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.4.w2.weight -> blk.19.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.19.block_sparse_moe.experts.4.w3.weight -> blk.19.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.5.w1.weight -> blk.19.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.5.w2.weight -> blk.19.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.19.block_sparse_moe.experts.5.w3.weight -> blk.19.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.6.w1.weight -> blk.19.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.6.w2.weight -> blk.19.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.19.block_sparse_moe.experts.6.w3.weight -> blk.19.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.7.w1.weight -> blk.19.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.experts.7.w2.weight -> blk.19.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.19.block_sparse_moe.experts.7.w3.weight -> blk.19.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.19.block_sparse_moe.gate.weight     -> blk.19.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.0.w1.weight -> blk.20.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.0.w2.weight -> blk.20.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.20.block_sparse_moe.experts.0.w3.weight -> blk.20.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.1.w1.weight -> blk.20.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.1.w2.weight -> blk.20.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.20.block_sparse_moe.experts.1.w3.weight -> blk.20.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.2.w1.weight -> blk.20.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.2.w2.weight -> blk.20.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.20.block_sparse_moe.experts.2.w3.weight -> blk.20.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.gate.weight     -> blk.20.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.3.w1.weight -> blk.20.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.3.w2.weight -> blk.20.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.20.block_sparse_moe.experts.3.w3.weight -> blk.20.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.4.w1.weight -> blk.20.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.4.w2.weight -> blk.20.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.20.block_sparse_moe.experts.4.w3.weight -> blk.20.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.5.w1.weight -> blk.20.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.5.w2.weight -> blk.20.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.20.block_sparse_moe.experts.5.w3.weight -> blk.20.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.6.w1.weight -> blk.20.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.6.w2.weight -> blk.20.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.20.block_sparse_moe.experts.6.w3.weight -> blk.20.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.7.w1.weight -> blk.20.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.20.block_sparse_moe.experts.7.w2.weight -> blk.20.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.20.block_sparse_moe.experts.7.w3.weight -> blk.20.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.21.block_sparse_moe.experts.0.w1.weight -> blk.21.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.0.w2.weight -> blk.21.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.21.block_sparse_moe.experts.0.w3.weight -> blk.21.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.1.w1.weight -> blk.21.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.1.w2.weight -> blk.21.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.21.block_sparse_moe.experts.1.w3.weight -> blk.21.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.2.w1.weight -> blk.21.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.2.w2.weight -> blk.21.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.21.block_sparse_moe.experts.2.w3.weight -> blk.21.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.3.w1.weight -> blk.21.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.3.w2.weight -> blk.21.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.21.block_sparse_moe.experts.3.w3.weight -> blk.21.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.4.w1.weight -> blk.21.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.4.w2.weight -> blk.21.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.21.block_sparse_moe.experts.4.w3.weight -> blk.21.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.5.w1.weight -> blk.21.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.5.w2.weight -> blk.21.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.21.block_sparse_moe.experts.5.w3.weight -> blk.21.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.6.w1.weight -> blk.21.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.6.w2.weight -> blk.21.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.21.block_sparse_moe.experts.6.w3.weight -> blk.21.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.7.w1.weight -> blk.21.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.experts.7.w2.weight -> blk.21.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.21.block_sparse_moe.experts.7.w3.weight -> blk.21.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.21.block_sparse_moe.gate.weight     -> blk.21.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.0.w1.weight -> blk.22.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.0.w2.weight -> blk.22.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.22.block_sparse_moe.gate.weight     -> blk.22.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.0.w3.weight -> blk.22.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.1.w1.weight -> blk.22.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.1.w2.weight -> blk.22.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.22.block_sparse_moe.experts.1.w3.weight -> blk.22.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.2.w1.weight -> blk.22.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.2.w2.weight -> blk.22.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.22.block_sparse_moe.experts.2.w3.weight -> blk.22.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.3.w1.weight -> blk.22.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.3.w2.weight -> blk.22.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.22.block_sparse_moe.experts.3.w3.weight -> blk.22.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.4.w1.weight -> blk.22.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.4.w2.weight -> blk.22.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.22.block_sparse_moe.experts.4.w3.weight -> blk.22.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.5.w1.weight -> blk.22.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.5.w2.weight -> blk.22.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.22.block_sparse_moe.experts.5.w3.weight -> blk.22.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.6.w1.weight -> blk.22.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.6.w2.weight -> blk.22.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.22.block_sparse_moe.experts.6.w3.weight -> blk.22.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.7.w1.weight -> blk.22.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.22.block_sparse_moe.experts.7.w2.weight -> blk.22.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.22.block_sparse_moe.experts.7.w3.weight -> blk.22.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.23.block_sparse_moe.experts.0.w1.weight -> blk.23.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.0.w2.weight -> blk.23.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.23.block_sparse_moe.experts.0.w3.weight -> blk.23.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.1.w1.weight -> blk.23.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.1.w2.weight -> blk.23.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.23.block_sparse_moe.experts.1.w3.weight -> blk.23.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.2.w1.weight -> blk.23.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.2.w2.weight -> blk.23.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.23.block_sparse_moe.experts.2.w3.weight -> blk.23.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.3.w1.weight -> blk.23.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.3.w2.weight -> blk.23.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.23.block_sparse_moe.experts.3.w3.weight -> blk.23.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.4.w1.weight -> blk.23.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.4.w2.weight -> blk.23.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.23.block_sparse_moe.experts.4.w3.weight -> blk.23.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.5.w1.weight -> blk.23.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.5.w2.weight -> blk.23.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.23.block_sparse_moe.experts.5.w3.weight -> blk.23.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.6.w1.weight -> blk.23.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.gate.weight     -> blk.23.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.6.w2.weight -> blk.23.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.23.block_sparse_moe.experts.6.w3.weight -> blk.23.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.7.w1.weight -> blk.23.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.23.block_sparse_moe.experts.7.w2.weight -> blk.23.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.23.block_sparse_moe.experts.7.w3.weight -> blk.23.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.24.block_sparse_moe.experts.0.w1.weight -> blk.24.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.0.w2.weight -> blk.24.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.24.block_sparse_moe.experts.0.w3.weight -> blk.24.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.1.w1.weight -> blk.24.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.1.w2.weight -> blk.24.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.24.block_sparse_moe.experts.1.w3.weight -> blk.24.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.2.w1.weight -> blk.24.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.2.w2.weight -> blk.24.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.24.block_sparse_moe.experts.2.w3.weight -> blk.24.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.3.w1.weight -> blk.24.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.3.w2.weight -> blk.24.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.24.block_sparse_moe.experts.3.w3.weight -> blk.24.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.4.w1.weight -> blk.24.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.4.w2.weight -> blk.24.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.24.block_sparse_moe.experts.4.w3.weight -> blk.24.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.5.w1.weight -> blk.24.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.5.w2.weight -> blk.24.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.24.block_sparse_moe.experts.5.w3.weight -> blk.24.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.6.w1.weight -> blk.24.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.6.w2.weight -> blk.24.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.24.block_sparse_moe.experts.6.w3.weight -> blk.24.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.7.w1.weight -> blk.24.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.experts.7.w2.weight -> blk.24.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.24.block_sparse_moe.experts.7.w3.weight -> blk.24.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.24.block_sparse_moe.gate.weight     -> blk.24.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.0.w1.weight -> blk.25.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.0.w2.weight -> blk.25.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.25.block_sparse_moe.experts.0.w3.weight -> blk.25.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.1.w1.weight -> blk.25.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.1.w2.weight -> blk.25.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.25.block_sparse_moe.experts.1.w3.weight -> blk.25.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.2.w1.weight -> blk.25.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.2.w2.weight -> blk.25.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.25.block_sparse_moe.experts.2.w3.weight -> blk.25.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.3.w1.weight -> blk.25.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.3.w2.weight -> blk.25.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.25.block_sparse_moe.experts.3.w3.weight -> blk.25.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.gate.weight     -> blk.25.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.4.w1.weight -> blk.25.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.4.w2.weight -> blk.25.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.25.block_sparse_moe.experts.4.w3.weight -> blk.25.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.5.w1.weight -> blk.25.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.5.w2.weight -> blk.25.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.25.block_sparse_moe.experts.5.w3.weight -> blk.25.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.6.w1.weight -> blk.25.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.6.w2.weight -> blk.25.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.25.block_sparse_moe.experts.6.w3.weight -> blk.25.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.7.w1.weight -> blk.25.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.25.block_sparse_moe.experts.7.w2.weight -> blk.25.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.25.block_sparse_moe.experts.7.w3.weight -> blk.25.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.26.block_sparse_moe.experts.0.w1.weight -> blk.26.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.0.w2.weight -> blk.26.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.26.block_sparse_moe.experts.0.w3.weight -> blk.26.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.1.w1.weight -> blk.26.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.1.w2.weight -> blk.26.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.26.block_sparse_moe.experts.1.w3.weight -> blk.26.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.2.w1.weight -> blk.26.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.2.w2.weight -> blk.26.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.26.block_sparse_moe.experts.2.w3.weight -> blk.26.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.3.w1.weight -> blk.26.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.3.w2.weight -> blk.26.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.26.block_sparse_moe.experts.3.w3.weight -> blk.26.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.4.w1.weight -> blk.26.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.4.w2.weight -> blk.26.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.26.block_sparse_moe.experts.4.w3.weight -> blk.26.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.5.w1.weight -> blk.26.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.5.w2.weight -> blk.26.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.26.block_sparse_moe.experts.5.w3.weight -> blk.26.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.6.w1.weight -> blk.26.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.6.w2.weight -> blk.26.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.26.block_sparse_moe.experts.6.w3.weight -> blk.26.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.7.w1.weight -> blk.26.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.experts.7.w2.weight -> blk.26.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.26.block_sparse_moe.experts.7.w3.weight -> blk.26.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.26.block_sparse_moe.gate.weight     -> blk.26.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.0.w1.weight -> blk.27.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.0.w2.weight -> blk.27.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.27.block_sparse_moe.experts.0.w3.weight -> blk.27.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.1.w1.weight -> blk.27.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.1.w2.weight -> blk.27.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.27.block_sparse_moe.gate.weight     -> blk.27.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.1.w3.weight -> blk.27.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.2.w1.weight -> blk.27.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.2.w2.weight -> blk.27.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.27.block_sparse_moe.experts.2.w3.weight -> blk.27.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.3.w1.weight -> blk.27.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.3.w2.weight -> blk.27.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.27.block_sparse_moe.experts.3.w3.weight -> blk.27.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.4.w1.weight -> blk.27.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.4.w2.weight -> blk.27.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.27.block_sparse_moe.experts.4.w3.weight -> blk.27.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.5.w1.weight -> blk.27.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.5.w2.weight -> blk.27.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.27.block_sparse_moe.experts.5.w3.weight -> blk.27.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.6.w1.weight -> blk.27.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.6.w2.weight -> blk.27.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.27.block_sparse_moe.experts.6.w3.weight -> blk.27.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.7.w1.weight -> blk.27.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.27.block_sparse_moe.experts.7.w2.weight -> blk.27.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.27.block_sparse_moe.experts.7.w3.weight -> blk.27.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.28.block_sparse_moe.experts.0.w1.weight -> blk.28.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.0.w2.weight -> blk.28.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.28.block_sparse_moe.experts.0.w3.weight -> blk.28.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.1.w1.weight -> blk.28.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.1.w2.weight -> blk.28.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.28.block_sparse_moe.experts.1.w3.weight -> blk.28.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.2.w1.weight -> blk.28.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.2.w2.weight -> blk.28.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.28.block_sparse_moe.experts.2.w3.weight -> blk.28.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.3.w1.weight -> blk.28.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.3.w2.weight -> blk.28.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.28.block_sparse_moe.experts.3.w3.weight -> blk.28.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.4.w1.weight -> blk.28.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.4.w2.weight -> blk.28.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.28.block_sparse_moe.experts.4.w3.weight -> blk.28.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.5.w1.weight -> blk.28.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.5.w2.weight -> blk.28.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.28.block_sparse_moe.experts.5.w3.weight -> blk.28.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.6.w1.weight -> blk.28.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.6.w2.weight -> blk.28.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.28.block_sparse_moe.experts.6.w3.weight -> blk.28.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.7.w1.weight -> blk.28.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.28.block_sparse_moe.gate.weight     -> blk.28.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.28.block_sparse_moe.experts.7.w2.weight -> blk.28.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.28.block_sparse_moe.experts.7.w3.weight -> blk.28.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.29.block_sparse_moe.experts.0.w1.weight -> blk.29.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.0.w2.weight -> blk.29.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.29.block_sparse_moe.experts.0.w3.weight -> blk.29.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.1.w1.weight -> blk.29.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.1.w2.weight -> blk.29.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.29.block_sparse_moe.experts.1.w3.weight -> blk.29.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.2.w1.weight -> blk.29.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.2.w2.weight -> blk.29.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.29.block_sparse_moe.experts.2.w3.weight -> blk.29.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.3.w1.weight -> blk.29.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.3.w2.weight -> blk.29.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.29.block_sparse_moe.experts.3.w3.weight -> blk.29.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.4.w1.weight -> blk.29.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.4.w2.weight -> blk.29.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.29.block_sparse_moe.experts.4.w3.weight -> blk.29.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.5.w1.weight -> blk.29.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.5.w2.weight -> blk.29.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.29.block_sparse_moe.experts.5.w3.weight -> blk.29.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.6.w1.weight -> blk.29.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.6.w2.weight -> blk.29.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.29.block_sparse_moe.experts.6.w3.weight -> blk.29.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.7.w1.weight -> blk.29.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.experts.7.w2.weight -> blk.29.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.29.block_sparse_moe.experts.7.w3.weight -> blk.29.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.29.block_sparse_moe.gate.weight     -> blk.29.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.0.w1.weight -> blk.30.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.0.w2.weight -> blk.30.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.30.block_sparse_moe.experts.0.w3.weight -> blk.30.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.1.w1.weight -> blk.30.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.1.w2.weight -> blk.30.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.30.block_sparse_moe.experts.1.w3.weight -> blk.30.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.2.w1.weight -> blk.30.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.2.w2.weight -> blk.30.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.30.block_sparse_moe.experts.2.w3.weight -> blk.30.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.3.w1.weight -> blk.30.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.3.w2.weight -> blk.30.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.30.block_sparse_moe.experts.3.w3.weight -> blk.30.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.4.w1.weight -> blk.30.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.4.w2.weight -> blk.30.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.30.block_sparse_moe.experts.4.w3.weight -> blk.30.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.gate.weight     -> blk.30.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "lm_head.weight                                   -> output.weight                            | BF16   | [32000, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.5.w1.weight -> blk.30.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.5.w2.weight -> blk.30.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.30.block_sparse_moe.experts.5.w3.weight -> blk.30.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.6.w1.weight -> blk.30.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.6.w2.weight -> blk.30.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.30.block_sparse_moe.experts.6.w3.weight -> blk.30.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.7.w1.weight -> blk.30.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.30.block_sparse_moe.experts.7.w2.weight -> blk.30.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.30.block_sparse_moe.experts.7.w3.weight -> blk.30.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.31.block_sparse_moe.experts.0.w1.weight -> blk.31.ffn_gate.0.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.0.w2.weight -> blk.31.ffn_down.0.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.31.block_sparse_moe.experts.0.w3.weight -> blk.31.ffn_up.0.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.1.w1.weight -> blk.31.ffn_gate.1.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.1.w2.weight -> blk.31.ffn_down.1.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.31.block_sparse_moe.experts.1.w3.weight -> blk.31.ffn_up.1.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.2.w1.weight -> blk.31.ffn_gate.2.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.2.w2.weight -> blk.31.ffn_down.2.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.31.block_sparse_moe.experts.2.w3.weight -> blk.31.ffn_up.2.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.3.w1.weight -> blk.31.ffn_gate.3.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.3.w2.weight -> blk.31.ffn_down.3.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.31.block_sparse_moe.experts.3.w3.weight -> blk.31.ffn_up.3.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.4.w1.weight -> blk.31.ffn_gate.4.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.4.w2.weight -> blk.31.ffn_down.4.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.31.block_sparse_moe.experts.4.w3.weight -> blk.31.ffn_up.4.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.5.w1.weight -> blk.31.ffn_gate.5.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.5.w2.weight -> blk.31.ffn_down.5.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.31.block_sparse_moe.experts.5.w3.weight -> blk.31.ffn_up.5.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.6.w1.weight -> blk.31.ffn_gate.6.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.6.w2.weight -> blk.31.ffn_down.6.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.31.block_sparse_moe.experts.6.w3.weight -> blk.31.ffn_up.6.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.7.w1.weight -> blk.31.ffn_gate.7.weight                 | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.experts.7.w2.weight -> blk.31.ffn_down.7.weight                 | BF16   | [4096, 14336]\n",
      "model.layers.31.block_sparse_moe.experts.7.w3.weight -> blk.31.ffn_up.7.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.31.block_sparse_moe.gate.weight     -> blk.31.ffn_gate_inp.weight               | BF16   | [8, 4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | BF16   | [4096]\n",
      "Writing mixtral-8x7b-instruct-v0.1.fp16.bin, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 58980 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "gguf: Setting chat_template to {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "[  1/995] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   1\n",
      "[  2/995] Writing tensor blk.0.ffn_gate.0.weight                | size  14336 x   4096  | type F16  | T+   1\n",
      "[  3/995] Writing tensor blk.0.ffn_down.0.weight                | size   4096 x  14336  | type F16  | T+   1\n",
      "[  4/995] Writing tensor blk.0.ffn_up.0.weight                  | size  14336 x   4096  | type F16  | T+   1\n",
      "[  5/995] Writing tensor blk.0.ffn_gate.1.weight                | size  14336 x   4096  | type F16  | T+   1\n",
      "[  6/995] Writing tensor blk.0.ffn_down.1.weight                | size   4096 x  14336  | type F16  | T+   1\n",
      "[  7/995] Writing tensor blk.0.ffn_up.1.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
      "[  8/995] Writing tensor blk.0.ffn_gate.2.weight                | size  14336 x   4096  | type F16  | T+   2\n",
      "[  9/995] Writing tensor blk.0.ffn_down.2.weight                | size   4096 x  14336  | type F16  | T+   2\n",
      "[ 10/995] Writing tensor blk.0.ffn_up.2.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 11/995] Writing tensor blk.0.ffn_gate.3.weight                | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 12/995] Writing tensor blk.0.ffn_down.3.weight                | size   4096 x  14336  | type F16  | T+   2\n",
      "[ 13/995] Writing tensor blk.0.ffn_up.3.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 14/995] Writing tensor blk.0.ffn_gate.4.weight                | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 15/995] Writing tensor blk.0.ffn_down.4.weight                | size   4096 x  14336  | type F16  | T+   2\n",
      "[ 16/995] Writing tensor blk.0.ffn_up.4.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 17/995] Writing tensor blk.0.ffn_gate.5.weight                | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 18/995] Writing tensor blk.0.ffn_down.5.weight                | size   4096 x  14336  | type F16  | T+   3\n",
      "[ 19/995] Writing tensor blk.0.ffn_up.5.weight                  | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 20/995] Writing tensor blk.0.ffn_gate.6.weight                | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 21/995] Writing tensor blk.0.ffn_down.6.weight                | size   4096 x  14336  | type F16  | T+   3\n",
      "[ 22/995] Writing tensor blk.0.ffn_up.6.weight                  | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 23/995] Writing tensor blk.0.ffn_gate.7.weight                | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 24/995] Writing tensor blk.0.ffn_down.7.weight                | size   4096 x  14336  | type F16  | T+   3\n",
      "[ 25/995] Writing tensor blk.0.ffn_up.7.weight                  | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 26/995] Writing tensor blk.0.ffn_gate_inp.weight              | size      8 x   4096  | type F16  | T+   3\n",
      "[ 27/995] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 28/995] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 29/995] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 30/995] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 31/995] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 32/995] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 33/995] Writing tensor blk.1.ffn_gate.0.weight                | size  14336 x   4096  | type F16  | T+   4\n",
      "[ 34/995] Writing tensor blk.1.ffn_down.0.weight                | size   4096 x  14336  | type F16  | T+   4\n",
      "[ 35/995] Writing tensor blk.1.ffn_up.0.weight                  | size  14336 x   4096  | type F16  | T+   4\n",
      "[ 36/995] Writing tensor blk.1.ffn_gate.1.weight                | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 37/995] Writing tensor blk.1.ffn_down.1.weight                | size   4096 x  14336  | type F16  | T+   5\n",
      "[ 38/995] Writing tensor blk.1.ffn_up.1.weight                  | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 39/995] Writing tensor blk.1.ffn_gate.2.weight                | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 40/995] Writing tensor blk.1.ffn_down.2.weight                | size   4096 x  14336  | type F16  | T+   5\n",
      "[ 41/995] Writing tensor blk.1.ffn_up.2.weight                  | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 42/995] Writing tensor blk.1.ffn_gate.3.weight                | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 43/995] Writing tensor blk.1.ffn_down.3.weight                | size   4096 x  14336  | type F16  | T+   5\n",
      "[ 44/995] Writing tensor blk.1.ffn_up.3.weight                  | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 45/995] Writing tensor blk.1.ffn_gate.4.weight                | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 46/995] Writing tensor blk.1.ffn_down.4.weight                | size   4096 x  14336  | type F16  | T+   5\n",
      "[ 47/995] Writing tensor blk.1.ffn_gate_inp.weight              | size      8 x   4096  | type F16  | T+   6\n",
      "[ 48/995] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   6\n",
      "[ 49/995] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   6\n",
      "[ 50/995] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   6\n",
      "[ 51/995] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   6\n",
      "[ 52/995] Writing tensor blk.1.ffn_up.4.weight                  | size  14336 x   4096  | type F16  | T+  10\n",
      "[ 53/995] Writing tensor blk.1.ffn_gate.5.weight                | size  14336 x   4096  | type F16  | T+  10\n",
      "[ 54/995] Writing tensor blk.1.ffn_down.5.weight                | size   4096 x  14336  | type F16  | T+  10\n",
      "[ 55/995] Writing tensor blk.1.ffn_up.5.weight                  | size  14336 x   4096  | type F16  | T+  10\n",
      "[ 56/995] Writing tensor blk.1.ffn_gate.6.weight                | size  14336 x   4096  | type F16  | T+  10\n",
      "[ 57/995] Writing tensor blk.1.ffn_down.6.weight                | size   4096 x  14336  | type F16  | T+  10\n",
      "[ 58/995] Writing tensor blk.1.ffn_up.6.weight                  | size  14336 x   4096  | type F16  | T+  11\n",
      "[ 59/995] Writing tensor blk.1.ffn_gate.7.weight                | size  14336 x   4096  | type F16  | T+  11\n",
      "[ 60/995] Writing tensor blk.1.ffn_down.7.weight                | size   4096 x  14336  | type F16  | T+  13\n",
      "[ 61/995] Writing tensor blk.1.ffn_up.7.weight                  | size  14336 x   4096  | type F16  | T+  13\n",
      "[ 62/995] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  14\n",
      "[ 63/995] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  14\n",
      "[ 64/995] Writing tensor blk.2.ffn_gate.0.weight                | size  14336 x   4096  | type F16  | T+  14\n",
      "[ 65/995] Writing tensor blk.2.ffn_down.0.weight                | size   4096 x  14336  | type F16  | T+  14\n",
      "[ 66/995] Writing tensor blk.2.ffn_up.0.weight                  | size  14336 x   4096  | type F16  | T+  15\n",
      "[ 67/995] Writing tensor blk.2.ffn_gate.1.weight                | size  14336 x   4096  | type F16  | T+  16\n",
      "[ 68/995] Writing tensor blk.2.ffn_down.1.weight                | size   4096 x  14336  | type F16  | T+  17\n",
      "[ 69/995] Writing tensor blk.2.ffn_up.1.weight                  | size  14336 x   4096  | type F16  | T+  18\n",
      "[ 70/995] Writing tensor blk.2.ffn_gate.2.weight                | size  14336 x   4096  | type F16  | T+  19\n",
      "[ 71/995] Writing tensor blk.2.ffn_down.2.weight                | size   4096 x  14336  | type F16  | T+  20\n",
      "[ 72/995] Writing tensor blk.2.ffn_up.2.weight                  | size  14336 x   4096  | type F16  | T+  20\n",
      "[ 73/995] Writing tensor blk.2.ffn_gate.3.weight                | size  14336 x   4096  | type F16  | T+  21\n",
      "[ 74/995] Writing tensor blk.2.ffn_down.3.weight                | size   4096 x  14336  | type F16  | T+  22\n",
      "[ 75/995] Writing tensor blk.2.ffn_up.3.weight                  | size  14336 x   4096  | type F16  | T+  23\n",
      "[ 76/995] Writing tensor blk.2.ffn_gate.4.weight                | size  14336 x   4096  | type F16  | T+  24\n",
      "[ 77/995] Writing tensor blk.2.ffn_down.4.weight                | size   4096 x  14336  | type F16  | T+  25\n",
      "[ 78/995] Writing tensor blk.2.ffn_up.4.weight                  | size  14336 x   4096  | type F16  | T+  26\n",
      "[ 79/995] Writing tensor blk.2.ffn_gate.5.weight                | size  14336 x   4096  | type F16  | T+  27\n",
      "[ 80/995] Writing tensor blk.2.ffn_down.5.weight                | size   4096 x  14336  | type F16  | T+  28\n",
      "[ 81/995] Writing tensor blk.2.ffn_up.5.weight                  | size  14336 x   4096  | type F16  | T+  29\n",
      "[ 82/995] Writing tensor blk.2.ffn_gate.6.weight                | size  14336 x   4096  | type F16  | T+  29\n",
      "[ 83/995] Writing tensor blk.2.ffn_down.6.weight                | size   4096 x  14336  | type F16  | T+  30\n",
      "[ 84/995] Writing tensor blk.2.ffn_up.6.weight                  | size  14336 x   4096  | type F16  | T+  31\n",
      "[ 85/995] Writing tensor blk.2.ffn_gate.7.weight                | size  14336 x   4096  | type F16  | T+  32\n",
      "[ 86/995] Writing tensor blk.2.ffn_down.7.weight                | size   4096 x  14336  | type F16  | T+  32\n",
      "[ 87/995] Writing tensor blk.2.ffn_up.7.weight                  | size  14336 x   4096  | type F16  | T+  33\n",
      "[ 88/995] Writing tensor blk.2.ffn_gate_inp.weight              | size      8 x   4096  | type F16  | T+  33\n",
      "[ 89/995] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  33\n",
      "[ 90/995] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  33\n",
      "[ 91/995] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  33\n",
      "[ 92/995] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+  33\n",
      "[ 93/995] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  33\n",
      "[ 94/995] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  33\n",
      "[ 95/995] Writing tensor blk.3.ffn_gate.0.weight                | size  14336 x   4096  | type F16  | T+  36\n",
      "[ 96/995] Writing tensor blk.3.ffn_down.0.weight                | size   4096 x  14336  | type F16  | T+  37\n",
      "[ 97/995] Writing tensor blk.3.ffn_up.0.weight                  | size  14336 x   4096  | type F16  | T+  37\n",
      "[ 98/995] Writing tensor blk.3.ffn_gate.1.weight                | size  14336 x   4096  | type F16  | T+  38\n",
      "[ 99/995] Writing tensor blk.3.ffn_down.1.weight                | size   4096 x  14336  | type F16  | T+  38\n",
      "[100/995] Writing tensor blk.3.ffn_up.1.weight                  | size  14336 x   4096  | type F16  | T+  39\n",
      "[101/995] Writing tensor blk.3.ffn_gate.2.weight                | size  14336 x   4096  | type F16  | T+  39\n",
      "[102/995] Writing tensor blk.3.ffn_gate_inp.weight              | size      8 x   4096  | type F16  | T+  40\n",
      "[103/995] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  40\n",
      "[104/995] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+  40\n",
      "[105/995] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  40\n",
      "[106/995] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  40\n",
      "[107/995] Writing tensor blk.3.ffn_down.2.weight                | size   4096 x  14336  | type F16  | T+  41\n",
      "[108/995] Writing tensor blk.3.ffn_up.2.weight                  | size  14336 x   4096  | type F16  | T+  42\n",
      "[109/995] Writing tensor blk.3.ffn_gate.3.weight                | size  14336 x   4096  | type F16  | T+  43\n",
      "[110/995] Writing tensor blk.3.ffn_down.3.weight                | size   4096 x  14336  | type F16  | T+  44\n",
      "[111/995] Writing tensor blk.3.ffn_up.3.weight                  | size  14336 x   4096  | type F16  | T+  45\n",
      "[112/995] Writing tensor blk.3.ffn_gate.4.weight                | size  14336 x   4096  | type F16  | T+  46\n",
      "[113/995] Writing tensor blk.3.ffn_down.4.weight                | size   4096 x  14336  | type F16  | T+  47\n",
      "[114/995] Writing tensor blk.3.ffn_up.4.weight                  | size  14336 x   4096  | type F16  | T+  49\n",
      "[115/995] Writing tensor blk.3.ffn_gate.5.weight                | size  14336 x   4096  | type F16  | T+  49\n",
      "[116/995] Writing tensor blk.3.ffn_down.5.weight                | size   4096 x  14336  | type F16  | T+  50\n",
      "[117/995] Writing tensor blk.3.ffn_up.5.weight                  | size  14336 x   4096  | type F16  | T+  51\n",
      "[118/995] Writing tensor blk.3.ffn_gate.6.weight                | size  14336 x   4096  | type F16  | T+  52\n",
      "[119/995] Writing tensor blk.3.ffn_down.6.weight                | size   4096 x  14336  | type F16  | T+  52\n",
      "[120/995] Writing tensor blk.3.ffn_up.6.weight                  | size  14336 x   4096  | type F16  | T+  53\n",
      "[121/995] Writing tensor blk.3.ffn_gate.7.weight                | size  14336 x   4096  | type F16  | T+  54\n",
      "[122/995] Writing tensor blk.3.ffn_down.7.weight                | size   4096 x  14336  | type F16  | T+  55\n",
      "[123/995] Writing tensor blk.3.ffn_up.7.weight                  | size  14336 x   4096  | type F16  | T+  56\n",
      "[124/995] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  57\n",
      "[125/995] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  57\n",
      "[126/995] Writing tensor blk.4.ffn_gate.0.weight                | size  14336 x   4096  | type F16  | T+  57\n",
      "[127/995] Writing tensor blk.4.ffn_down.0.weight                | size   4096 x  14336  | type F16  | T+  58\n",
      "[128/995] Writing tensor blk.4.ffn_up.0.weight                  | size  14336 x   4096  | type F16  | T+  59\n",
      "[129/995] Writing tensor blk.4.ffn_gate.1.weight                | size  14336 x   4096  | type F16  | T+  60\n",
      "[130/995] Writing tensor blk.4.ffn_down.1.weight                | size   4096 x  14336  | type F16  | T+  61\n",
      "[131/995] Writing tensor blk.4.ffn_up.1.weight                  | size  14336 x   4096  | type F16  | T+  62\n",
      "[132/995] Writing tensor blk.4.ffn_gate.2.weight                | size  14336 x   4096  | type F16  | T+  63\n",
      "[133/995] Writing tensor blk.4.ffn_down.2.weight                | size   4096 x  14336  | type F16  | T+  64\n",
      "[134/995] Writing tensor blk.4.ffn_up.2.weight                  | size  14336 x   4096  | type F16  | T+  65\n",
      "[135/995] Writing tensor blk.4.ffn_gate.3.weight                | size  14336 x   4096  | type F16  | T+  65\n",
      "[136/995] Writing tensor blk.4.ffn_down.3.weight                | size   4096 x  14336  | type F16  | T+  66\n",
      "[137/995] Writing tensor blk.4.ffn_up.3.weight                  | size  14336 x   4096  | type F16  | T+  67\n",
      "[138/995] Writing tensor blk.4.ffn_gate.4.weight                | size  14336 x   4096  | type F16  | T+  68\n",
      "[139/995] Writing tensor blk.4.ffn_down.4.weight                | size   4096 x  14336  | type F16  | T+  69\n",
      "[140/995] Writing tensor blk.4.ffn_up.4.weight                  | size  14336 x   4096  | type F16  | T+  70\n",
      "[141/995] Writing tensor blk.4.ffn_gate.5.weight                | size  14336 x   4096  | type F16  | T+  71\n",
      "[142/995] Writing tensor blk.4.ffn_down.5.weight                | size   4096 x  14336  | type F16  | T+  72\n",
      "[143/995] Writing tensor blk.4.ffn_up.5.weight                  | size  14336 x   4096  | type F16  | T+  72\n",
      "[144/995] Writing tensor blk.4.ffn_gate.6.weight                | size  14336 x   4096  | type F16  | T+  73\n",
      "[145/995] Writing tensor blk.4.ffn_down.6.weight                | size   4096 x  14336  | type F16  | T+  74\n",
      "[146/995] Writing tensor blk.4.ffn_up.6.weight                  | size  14336 x   4096  | type F16  | T+  75\n",
      "[147/995] Writing tensor blk.4.ffn_gate.7.weight                | size  14336 x   4096  | type F16  | T+  75\n",
      "[148/995] Writing tensor blk.4.ffn_down.7.weight                | size   4096 x  14336  | type F16  | T+  76\n",
      "[149/995] Writing tensor blk.4.ffn_up.7.weight                  | size  14336 x   4096  | type F16  | T+  76\n",
      "[150/995] Writing tensor blk.4.ffn_gate_inp.weight              | size      8 x   4096  | type F16  | T+  77\n",
      "[151/995] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  77\n",
      "[152/995] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  77\n",
      "[153/995] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  77\n",
      "[154/995] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+  77\n",
      "[155/995] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  77\n",
      "[156/995] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  77\n",
      "[157/995] Writing tensor blk.5.ffn_gate_inp.weight              | size      8 x   4096  | type F16  | T+  77\n",
      "[158/995] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  77\n",
      "[159/995] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  78\n",
      "[160/995] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  78\n",
      "[161/995] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  78\n",
      "[162/995] Writing tensor blk.5.ffn_gate.0.weight                | size  14336 x   4096  | type F16  | T+  81\n",
      "[163/995] Writing tensor blk.5.ffn_down.0.weight                | size   4096 x  14336  | type F16  | T+  82\n",
      "[164/995] Writing tensor blk.5.ffn_up.0.weight                  | size  14336 x   4096  | type F16  | T+  82\n",
      "[165/995] Writing tensor blk.5.ffn_gate.1.weight                | size  14336 x   4096  | type F16  | T+  83\n",
      "[166/995] Writing tensor blk.5.ffn_down.1.weight                | size   4096 x  14336  | type F16  | T+  83\n",
      "[167/995] Writing tensor blk.5.ffn_up.1.weight                  | size  14336 x   4096  | type F16  | T+  84\n",
      "[168/995] Writing tensor blk.5.ffn_gate.2.weight                | size  14336 x   4096  | type F16  | T+  84\n",
      "[169/995] Writing tensor blk.5.ffn_down.2.weight                | size   4096 x  14336  | type F16  | T+  85\n",
      "[170/995] Writing tensor blk.5.ffn_up.2.weight                  | size  14336 x   4096  | type F16  | T+  86\n",
      "[171/995] Writing tensor blk.5.ffn_gate.3.weight                | size  14336 x   4096  | type F16  | T+  87\n",
      "[172/995] Writing tensor blk.5.ffn_down.3.weight                | size   4096 x  14336  | type F16  | T+  88\n",
      "[173/995] Writing tensor blk.5.ffn_up.3.weight                  | size  14336 x   4096  | type F16  | T+  89\n",
      "[174/995] Writing tensor blk.5.ffn_gate.4.weight                | size  14336 x   4096  | type F16  | T+  90\n",
      "[175/995] Writing tensor blk.5.ffn_down.4.weight                | size   4096 x  14336  | type F16  | T+  91\n",
      "[176/995] Writing tensor blk.5.ffn_up.4.weight                  | size  14336 x   4096  | type F16  | T+  92\n",
      "[177/995] Writing tensor blk.5.ffn_gate.5.weight                | size  14336 x   4096  | type F16  | T+  93\n",
      "[178/995] Writing tensor blk.5.ffn_down.5.weight                | size   4096 x  14336  | type F16  | T+  94\n",
      "[179/995] Writing tensor blk.5.ffn_up.5.weight                  | size  14336 x   4096  | type F16  | T+  95\n",
      "[180/995] Writing tensor blk.5.ffn_gate.6.weight                | size  14336 x   4096  | type F16  | T+  95\n",
      "[181/995] Writing tensor blk.5.ffn_down.6.weight                | size   4096 x  14336  | type F16  | T+  96\n",
      "[182/995] Writing tensor blk.5.ffn_up.6.weight                  | size  14336 x   4096  | type F16  | T+  97\n",
      "[183/995] Writing tensor blk.5.ffn_gate.7.weight                | size  14336 x   4096  | type F16  | T+  98\n",
      "[184/995] Writing tensor blk.5.ffn_down.7.weight                | size   4096 x  14336  | type F16  | T+  99\n",
      "[185/995] Writing tensor blk.5.ffn_up.7.weight                  | size  14336 x   4096  | type F16  | T+  99\n",
      "[186/995] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+ 100\n",
      "[187/995] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+ 100\n",
      "[188/995] Writing tensor blk.6.ffn_gate.0.weight                | size  14336 x   4096  | type F16  | T+ 100\n",
      "[189/995] Writing tensor blk.6.ffn_down.0.weight                | size   4096 x  14336  | type F16  | T+ 101\n",
      "[190/995] Writing tensor blk.6.ffn_up.0.weight                  | size  14336 x   4096  | type F16  | T+ 102\n",
      "[191/995] Writing tensor blk.6.ffn_gate.1.weight                | size  14336 x   4096  | type F16  | T+ 103\n",
      "[192/995] Writing tensor blk.6.ffn_down.1.weight                | size   4096 x  14336  | type F16  | T+ 104\n",
      "[193/995] Writing tensor blk.6.ffn_up.1.weight                  | size  14336 x   4096  | type F16  | T+ 105\n",
      "[194/995] Writing tensor blk.6.ffn_gate.2.weight                | size  14336 x   4096  | type F16  | T+ 106\n",
      "[195/995] Writing tensor blk.6.ffn_down.2.weight                | size   4096 x  14336  | type F16  | T+ 107\n",
      "[196/995] Writing tensor blk.6.ffn_up.2.weight                  | size  14336 x   4096  | type F16  | T+ 108\n",
      "[197/995] Writing tensor blk.6.ffn_gate.3.weight                | size  14336 x   4096  | type F16  | T+ 109\n",
      "[198/995] Writing tensor blk.6.ffn_down.3.weight                | size   4096 x  14336  | type F16  | T+ 110\n",
      "[199/995] Writing tensor blk.6.ffn_up.3.weight                  | size  14336 x   4096  | type F16  | T+ 110\n",
      "[200/995] Writing tensor blk.6.ffn_gate.4.weight                | size  14336 x   4096  | type F16  | T+ 111\n",
      "[201/995] Writing tensor blk.6.ffn_down.4.weight                | size   4096 x  14336  | type F16  | T+ 112\n",
      "[202/995] Writing tensor blk.6.ffn_up.4.weight                  | size  14336 x   4096  | type F16  | T+ 112\n",
      "[203/995] Writing tensor blk.6.ffn_gate.5.weight                | size  14336 x   4096  | type F16  | T+ 113\n",
      "[204/995] Writing tensor blk.6.ffn_down.5.weight                | size   4096 x  14336  | type F16  | T+ 114\n",
      "[205/995] Writing tensor blk.6.ffn_gate_inp.weight              | size      8 x   4096  | type F16  | T+ 115\n",
      "[206/995] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F16  | T+ 115\n",
      "[207/995] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 115\n",
      "[208/995] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 116\n",
      "[209/995] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F16  | T+ 116\n",
      "[210/995] Writing tensor blk.6.ffn_up.5.weight                  | size  14336 x   4096  | type F16  | T+ 116\n",
      "[211/995] Writing tensor blk.6.ffn_gate.6.weight                | size  14336 x   4096  | type F16  | T+ 117\n",
      "[212/995] Writing tensor blk.6.ffn_down.6.weight                | size   4096 x  14336  | type F16  | T+ 118\n",
      "[213/995] Writing tensor blk.6.ffn_up.6.weight                  | size  14336 x   4096  | type F16  | T+ 120\n",
      "[214/995] Writing tensor blk.6.ffn_gate.7.weight                | size  14336 x   4096  | type F16  | T+ 121\n",
      "[215/995] Writing tensor blk.6.ffn_down.7.weight                | size   4096 x  14336  | type F16  | T+ 122\n",
      "[216/995] Writing tensor blk.6.ffn_up.7.weight                  | size  14336 x   4096  | type F16  | T+ 123\n",
      "[217/995] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+ 124\n",
      "[218/995] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+ 124\n",
      "[219/995] Writing tensor blk.7.ffn_gate.0.weight                | size  14336 x   4096  | type F16  | T+ 124\n",
      "[220/995] Writing tensor blk.7.ffn_down.0.weight                | size   4096 x  14336  | type F16  | T+ 124\n",
      "[221/995] Writing tensor blk.7.ffn_up.0.weight                  | size  14336 x   4096  | type F16  | T+ 125\n",
      "[222/995] Writing tensor blk.7.ffn_gate.1.weight                | size  14336 x   4096  | type F16  | T+ 127\n",
      "[223/995] Writing tensor blk.7.ffn_down.1.weight                | size   4096 x  14336  | type F16  | T+ 128\n",
      "[224/995] Writing tensor blk.7.ffn_up.1.weight                  | size  14336 x   4096  | type F16  | T+ 129\n",
      "[225/995] Writing tensor blk.7.ffn_gate.2.weight                | size  14336 x   4096  | type F16  | T+ 130\n",
      "[226/995] Writing tensor blk.7.ffn_down.2.weight                | size   4096 x  14336  | type F16  | T+ 130\n",
      "[227/995] Writing tensor blk.7.ffn_up.2.weight                  | size  14336 x   4096  | type F16  | T+ 131\n",
      "[228/995] Writing tensor blk.7.ffn_gate.3.weight                | size  14336 x   4096  | type F16  | T+ 132\n",
      "[229/995] Writing tensor blk.7.ffn_down.3.weight                | size   4096 x  14336  | type F16  | T+ 133\n",
      "[230/995] Writing tensor blk.7.ffn_up.3.weight                  | size  14336 x   4096  | type F16  | T+ 134\n",
      "[231/995] Writing tensor blk.7.ffn_gate.4.weight                | size  14336 x   4096  | type F16  | T+ 135\n",
      "[232/995] Writing tensor blk.7.ffn_down.4.weight                | size   4096 x  14336  | type F16  | T+ 136\n",
      "[233/995] Writing tensor blk.7.ffn_up.4.weight                  | size  14336 x   4096  | type F16  | T+ 137\n",
      "[234/995] Writing tensor blk.7.ffn_gate.5.weight                | size  14336 x   4096  | type F16  | T+ 138\n",
      "[235/995] Writing tensor blk.7.ffn_down.5.weight                | size   4096 x  14336  | type F16  | T+ 138\n",
      "[236/995] Writing tensor blk.7.ffn_up.5.weight                  | size  14336 x   4096  | type F16  | T+ 139\n",
      "[237/995] Writing tensor blk.7.ffn_gate.6.weight                | size  14336 x   4096  | type F16  | T+ 140\n",
      "[238/995] Writing tensor blk.7.ffn_down.6.weight                | size   4096 x  14336  | type F16  | T+ 141\n",
      "[239/995] Writing tensor blk.7.ffn_up.6.weight                  | size  14336 x   4096  | type F16  | T+ 142\n",
      "[240/995] Writing tensor blk.7.ffn_gate.7.weight                | size  14336 x   4096  | type F16  | T+ 142\n",
      "[241/995] Writing tensor blk.7.ffn_down.7.weight                | size   4096 x  14336  | type F16  | T+ 143\n",
      "[242/995] Writing tensor blk.7.ffn_up.7.weight                  | size  14336 x   4096  | type F16  | T+ 143\n",
      "[243/995] Writing tensor blk.7.ffn_gate_inp.weight              | size      8 x   4096  | type F16  | T+ 144\n",
      "[244/995] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+ 144\n",
      "[245/995] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+ 144\n",
      "[246/995] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F16  | T+ 144\n",
      "[247/995] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 144\n",
      "[248/995] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 144\n",
      "[249/995] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F16  | T+ 144\n",
      "[250/995] Writing tensor blk.8.ffn_gate.0.weight                | size  14336 x   4096  | type F16  | T+ 147\n",
      "[251/995] Writing tensor blk.8.ffn_down.0.weight                | size   4096 x  14336  | type F16  | T+ 149\n",
      "[252/995] Writing tensor blk.8.ffn_up.0.weight                  | size  14336 x   4096  | type F16  | T+ 149\n",
      "[253/995] Writing tensor blk.8.ffn_gate.1.weight                | size  14336 x   4096  | type F16  | T+ 149\n",
      "[254/995] Writing tensor blk.8.ffn_down.1.weight                | size   4096 x  14336  | type F16  | T+ 150\n",
      "[255/995] Writing tensor blk.8.ffn_up.1.weight                  | size  14336 x   4096  | type F16  | T+ 150\n",
      "[256/995] Writing tensor blk.8.ffn_gate.2.weight                | size  14336 x   4096  | type F16  | T+ 151\n",
      "[257/995] Writing tensor blk.8.ffn_down.2.weight                | size   4096 x  14336  | type F16  | T+ 152\n",
      "[258/995] Writing tensor blk.8.ffn_up.2.weight                  | size  14336 x   4096  | type F16  | T+ 152\n",
      "[259/995] Writing tensor blk.8.ffn_gate.3.weight                | size  14336 x   4096  | type F16  | T+ 153\n",
      "[260/995] Writing tensor blk.8.ffn_gate_inp.weight              | size      8 x   4096  | type F16  | T+ 154\n",
      "[261/995] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F16  | T+ 154\n",
      "[262/995] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 154\n",
      "[263/995] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 154\n",
      "[264/995] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F16  | T+ 154\n",
      "[265/995] Writing tensor blk.10.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 154\n",
      "[266/995] Writing tensor blk.10.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 155\n",
      "[267/995] Writing tensor blk.10.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 155\n",
      "[268/995] Writing tensor blk.10.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 156\n",
      "[269/995] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 156\n",
      "[270/995] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 156\n",
      "[271/995] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 156\n",
      "[272/995] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 157\n",
      "[273/995] Writing tensor blk.8.ffn_down.3.weight                | size   4096 x  14336  | type F16  | T+ 157\n",
      "[274/995] Writing tensor blk.8.ffn_up.3.weight                  | size  14336 x   4096  | type F16  | T+ 158\n",
      "[275/995] Writing tensor blk.8.ffn_gate.4.weight                | size  14336 x   4096  | type F16  | T+ 159\n",
      "[276/995] Writing tensor blk.8.ffn_down.4.weight                | size   4096 x  14336  | type F16  | T+ 161\n",
      "[277/995] Writing tensor blk.8.ffn_up.4.weight                  | size  14336 x   4096  | type F16  | T+ 162\n",
      "[278/995] Writing tensor blk.8.ffn_gate.5.weight                | size  14336 x   4096  | type F16  | T+ 163\n",
      "[279/995] Writing tensor blk.8.ffn_down.5.weight                | size   4096 x  14336  | type F16  | T+ 164\n",
      "[280/995] Writing tensor blk.8.ffn_up.5.weight                  | size  14336 x   4096  | type F16  | T+ 165\n",
      "[281/995] Writing tensor blk.8.ffn_gate.6.weight                | size  14336 x   4096  | type F16  | T+ 166\n",
      "[282/995] Writing tensor blk.8.ffn_down.6.weight                | size   4096 x  14336  | type F16  | T+ 167\n",
      "[283/995] Writing tensor blk.8.ffn_up.6.weight                  | size  14336 x   4096  | type F16  | T+ 168\n",
      "[284/995] Writing tensor blk.8.ffn_gate.7.weight                | size  14336 x   4096  | type F16  | T+ 169\n",
      "[285/995] Writing tensor blk.8.ffn_down.7.weight                | size   4096 x  14336  | type F16  | T+ 170\n",
      "[286/995] Writing tensor blk.8.ffn_up.7.weight                  | size  14336 x   4096  | type F16  | T+ 170\n",
      "[287/995] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+ 171\n",
      "[288/995] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+ 171\n",
      "[289/995] Writing tensor blk.9.ffn_gate.0.weight                | size  14336 x   4096  | type F16  | T+ 171\n",
      "[290/995] Writing tensor blk.9.ffn_down.0.weight                | size   4096 x  14336  | type F16  | T+ 172\n",
      "[291/995] Writing tensor blk.9.ffn_up.0.weight                  | size  14336 x   4096  | type F16  | T+ 173\n",
      "[292/995] Writing tensor blk.9.ffn_gate.1.weight                | size  14336 x   4096  | type F16  | T+ 174\n",
      "[293/995] Writing tensor blk.9.ffn_down.1.weight                | size   4096 x  14336  | type F16  | T+ 175\n",
      "[294/995] Writing tensor blk.9.ffn_up.1.weight                  | size  14336 x   4096  | type F16  | T+ 176\n",
      "[295/995] Writing tensor blk.9.ffn_gate.2.weight                | size  14336 x   4096  | type F16  | T+ 177\n",
      "[296/995] Writing tensor blk.9.ffn_down.2.weight                | size   4096 x  14336  | type F16  | T+ 178\n",
      "[297/995] Writing tensor blk.9.ffn_up.2.weight                  | size  14336 x   4096  | type F16  | T+ 179\n",
      "[298/995] Writing tensor blk.9.ffn_gate.3.weight                | size  14336 x   4096  | type F16  | T+ 180\n",
      "[299/995] Writing tensor blk.9.ffn_down.3.weight                | size   4096 x  14336  | type F16  | T+ 181\n",
      "[300/995] Writing tensor blk.9.ffn_up.3.weight                  | size  14336 x   4096  | type F16  | T+ 181\n",
      "[301/995] Writing tensor blk.9.ffn_gate.4.weight                | size  14336 x   4096  | type F16  | T+ 182\n",
      "[302/995] Writing tensor blk.9.ffn_down.4.weight                | size   4096 x  14336  | type F16  | T+ 183\n",
      "[303/995] Writing tensor blk.9.ffn_up.4.weight                  | size  14336 x   4096  | type F16  | T+ 184\n",
      "[304/995] Writing tensor blk.9.ffn_gate.5.weight                | size  14336 x   4096  | type F16  | T+ 185\n",
      "[305/995] Writing tensor blk.9.ffn_down.5.weight                | size   4096 x  14336  | type F16  | T+ 186\n",
      "[306/995] Writing tensor blk.9.ffn_up.5.weight                  | size  14336 x   4096  | type F16  | T+ 187\n",
      "[307/995] Writing tensor blk.9.ffn_gate.6.weight                | size  14336 x   4096  | type F16  | T+ 187\n",
      "[308/995] Writing tensor blk.9.ffn_down.6.weight                | size   4096 x  14336  | type F16  | T+ 188\n",
      "[309/995] Writing tensor blk.9.ffn_up.6.weight                  | size  14336 x   4096  | type F16  | T+ 189\n",
      "[310/995] Writing tensor blk.9.ffn_gate.7.weight                | size  14336 x   4096  | type F16  | T+ 189\n",
      "[311/995] Writing tensor blk.9.ffn_down.7.weight                | size   4096 x  14336  | type F16  | T+ 190\n",
      "[312/995] Writing tensor blk.9.ffn_up.7.weight                  | size  14336 x   4096  | type F16  | T+ 191\n",
      "[313/995] Writing tensor blk.9.ffn_gate_inp.weight              | size      8 x   4096  | type F16  | T+ 191\n",
      "[314/995] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+ 191\n",
      "[315/995] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+ 191\n",
      "[316/995] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F16  | T+ 191\n",
      "[317/995] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 191\n",
      "[318/995] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 191\n",
      "[319/995] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F16  | T+ 191\n",
      "[320/995] Writing tensor blk.10.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 195\n",
      "[321/995] Writing tensor blk.10.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 196\n",
      "[322/995] Writing tensor blk.10.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 196\n",
      "[323/995] Writing tensor blk.10.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 197\n",
      "[324/995] Writing tensor blk.10.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 197\n",
      "[325/995] Writing tensor blk.10.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 198\n",
      "[326/995] Writing tensor blk.10.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 198\n",
      "[327/995] Writing tensor blk.10.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 199\n",
      "[328/995] Writing tensor blk.10.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 200\n",
      "[329/995] Writing tensor blk.10.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 201\n",
      "[330/995] Writing tensor blk.10.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 203\n",
      "[331/995] Writing tensor blk.10.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 204\n",
      "[332/995] Writing tensor blk.10.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 204\n",
      "[333/995] Writing tensor blk.10.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 205\n",
      "[334/995] Writing tensor blk.10.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 206\n",
      "[335/995] Writing tensor blk.10.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 207\n",
      "[336/995] Writing tensor blk.10.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 207\n",
      "[337/995] Writing tensor blk.10.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 208\n",
      "[338/995] Writing tensor blk.10.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 209\n",
      "[339/995] Writing tensor blk.10.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 210\n",
      "[340/995] Writing tensor blk.10.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 211\n",
      "[341/995] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+ 212\n",
      "[342/995] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+ 212\n",
      "[343/995] Writing tensor blk.11.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 212\n",
      "[344/995] Writing tensor blk.11.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 213\n",
      "[345/995] Writing tensor blk.11.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 214\n",
      "[346/995] Writing tensor blk.11.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 215\n",
      "[347/995] Writing tensor blk.11.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 216\n",
      "[348/995] Writing tensor blk.11.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 217\n",
      "[349/995] Writing tensor blk.11.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 218\n",
      "[350/995] Writing tensor blk.11.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 219\n",
      "[351/995] Writing tensor blk.11.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 220\n",
      "[352/995] Writing tensor blk.11.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 221\n",
      "[353/995] Writing tensor blk.11.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 222\n",
      "[354/995] Writing tensor blk.11.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 222\n",
      "[355/995] Writing tensor blk.11.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 223\n",
      "[356/995] Writing tensor blk.11.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 224\n",
      "[357/995] Writing tensor blk.11.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 225\n",
      "[358/995] Writing tensor blk.11.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 225\n",
      "[359/995] Writing tensor blk.11.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 226\n",
      "[360/995] Writing tensor blk.11.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 227\n",
      "[361/995] Writing tensor blk.11.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 228\n",
      "[362/995] Writing tensor blk.11.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 228\n",
      "[363/995] Writing tensor blk.11.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 229\n",
      "[364/995] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 229\n",
      "[365/995] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 229\n",
      "[366/995] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 229\n",
      "[367/995] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 229\n",
      "[368/995] Writing tensor blk.11.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 229\n",
      "[369/995] Writing tensor blk.11.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 231\n",
      "[370/995] Writing tensor blk.11.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 231\n",
      "[371/995] Writing tensor blk.11.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 233\n",
      "[372/995] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+ 233\n",
      "[373/995] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+ 233\n",
      "[374/995] Writing tensor blk.12.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 233\n",
      "[375/995] Writing tensor blk.12.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 234\n",
      "[376/995] Writing tensor blk.12.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 235\n",
      "[377/995] Writing tensor blk.12.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 237\n",
      "[378/995] Writing tensor blk.12.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 238\n",
      "[379/995] Writing tensor blk.12.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 239\n",
      "[380/995] Writing tensor blk.12.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 240\n",
      "[381/995] Writing tensor blk.12.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 241\n",
      "[382/995] Writing tensor blk.12.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 242\n",
      "[383/995] Writing tensor blk.12.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 243\n",
      "[384/995] Writing tensor blk.12.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 244\n",
      "[385/995] Writing tensor blk.12.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 245\n",
      "[386/995] Writing tensor blk.12.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 245\n",
      "[387/995] Writing tensor blk.12.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 246\n",
      "[388/995] Writing tensor blk.12.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 247\n",
      "[389/995] Writing tensor blk.12.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 248\n",
      "[390/995] Writing tensor blk.12.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 249\n",
      "[391/995] Writing tensor blk.12.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 250\n",
      "[392/995] Writing tensor blk.12.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 250\n",
      "[393/995] Writing tensor blk.12.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 251\n",
      "[394/995] Writing tensor blk.12.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 252\n",
      "[395/995] Writing tensor blk.12.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 253\n",
      "[396/995] Writing tensor blk.12.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 253\n",
      "[397/995] Writing tensor blk.12.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 254\n",
      "[398/995] Writing tensor blk.12.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 254\n",
      "[399/995] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+ 254\n",
      "[400/995] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+ 254\n",
      "[401/995] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 254\n",
      "[402/995] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 254\n",
      "[403/995] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 255\n",
      "[404/995] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 255\n",
      "[405/995] Writing tensor blk.13.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 258\n",
      "[406/995] Writing tensor blk.13.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 259\n",
      "[407/995] Writing tensor blk.13.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 259\n",
      "[408/995] Writing tensor blk.13.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 260\n",
      "[409/995] Writing tensor blk.13.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 260\n",
      "[410/995] Writing tensor blk.13.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 261\n",
      "[411/995] Writing tensor blk.13.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 261\n",
      "[412/995] Writing tensor blk.13.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 262\n",
      "[413/995] Writing tensor blk.13.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 263\n",
      "[414/995] Writing tensor blk.13.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 263\n",
      "[415/995] Writing tensor blk.13.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 264\n",
      "[416/995] Writing tensor blk.13.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 264\n",
      "[417/995] Writing tensor blk.13.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 265\n",
      "[418/995] Writing tensor blk.13.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 266\n",
      "[419/995] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 266\n",
      "[420/995] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 266\n",
      "[421/995] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 266\n",
      "[422/995] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 267\n",
      "[423/995] Writing tensor blk.13.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 267\n",
      "[424/995] Writing tensor blk.13.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 268\n",
      "[425/995] Writing tensor blk.13.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 269\n",
      "[426/995] Writing tensor blk.13.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 271\n",
      "[427/995] Writing tensor blk.13.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 272\n",
      "[428/995] Writing tensor blk.13.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 273\n",
      "[429/995] Writing tensor blk.13.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 274\n",
      "[430/995] Writing tensor blk.13.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 275\n",
      "[431/995] Writing tensor blk.13.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 276\n",
      "[432/995] Writing tensor blk.13.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 277\n",
      "[433/995] Writing tensor blk.13.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 278\n",
      "[434/995] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+ 278\n",
      "[435/995] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+ 278\n",
      "[436/995] Writing tensor blk.14.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 278\n",
      "[437/995] Writing tensor blk.14.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 279\n",
      "[438/995] Writing tensor blk.14.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 280\n",
      "[439/995] Writing tensor blk.14.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 281\n",
      "[440/995] Writing tensor blk.14.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 282\n",
      "[441/995] Writing tensor blk.14.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 283\n",
      "[442/995] Writing tensor blk.14.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 284\n",
      "[443/995] Writing tensor blk.14.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 285\n",
      "[444/995] Writing tensor blk.14.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 286\n",
      "[445/995] Writing tensor blk.14.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 287\n",
      "[446/995] Writing tensor blk.14.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 288\n",
      "[447/995] Writing tensor blk.14.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 289\n",
      "[448/995] Writing tensor blk.14.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 290\n",
      "[449/995] Writing tensor blk.14.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 290\n",
      "[450/995] Writing tensor blk.14.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 291\n",
      "[451/995] Writing tensor blk.14.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 292\n",
      "[452/995] Writing tensor blk.14.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 293\n",
      "[453/995] Writing tensor blk.14.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 294\n",
      "[454/995] Writing tensor blk.14.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 295\n",
      "[455/995] Writing tensor blk.14.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 295\n",
      "[456/995] Writing tensor blk.14.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 296\n",
      "[457/995] Writing tensor blk.14.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 297\n",
      "[458/995] Writing tensor blk.14.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 297\n",
      "[459/995] Writing tensor blk.14.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 298\n",
      "[460/995] Writing tensor blk.14.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 298\n",
      "[461/995] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+ 298\n",
      "[462/995] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+ 298\n",
      "[463/995] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 298\n",
      "[464/995] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 299\n",
      "[465/995] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 299\n",
      "[466/995] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 299\n",
      "[467/995] Writing tensor blk.15.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 301\n",
      "[468/995] Writing tensor blk.15.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 302\n",
      "[469/995] Writing tensor blk.15.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 303\n",
      "[470/995] Writing tensor blk.15.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 303\n",
      "[471/995] Writing tensor blk.15.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 304\n",
      "[472/995] Writing tensor blk.15.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 304\n",
      "[473/995] Writing tensor blk.15.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 305\n",
      "[474/995] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 305\n",
      "[475/995] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 305\n",
      "[476/995] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 305\n",
      "[477/995] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 305\n",
      "[478/995] Writing tensor blk.15.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 305\n",
      "[479/995] Writing tensor blk.15.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 307\n",
      "[480/995] Writing tensor blk.15.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 308\n",
      "[481/995] Writing tensor blk.15.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 309\n",
      "[482/995] Writing tensor blk.15.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 310\n",
      "[483/995] Writing tensor blk.15.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 311\n",
      "[484/995] Writing tensor blk.15.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 312\n",
      "[485/995] Writing tensor blk.15.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 313\n",
      "[486/995] Writing tensor blk.15.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 314\n",
      "[487/995] Writing tensor blk.15.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 315\n",
      "[488/995] Writing tensor blk.15.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 316\n",
      "[489/995] Writing tensor blk.15.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 318\n",
      "[490/995] Writing tensor blk.15.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 319\n",
      "[491/995] Writing tensor blk.15.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 321\n",
      "[492/995] Writing tensor blk.15.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 322\n",
      "[493/995] Writing tensor blk.15.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 324\n",
      "[494/995] Writing tensor blk.15.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 325\n",
      "[495/995] Writing tensor blk.15.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 327\n",
      "[496/995] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+ 329\n",
      "[497/995] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+ 329\n",
      "[498/995] Writing tensor blk.16.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 329\n",
      "[499/995] Writing tensor blk.16.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 330\n",
      "[500/995] Writing tensor blk.16.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 332\n",
      "[501/995] Writing tensor blk.16.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 334\n",
      "[502/995] Writing tensor blk.16.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 335\n",
      "[503/995] Writing tensor blk.16.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 337\n",
      "[504/995] Writing tensor blk.16.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 338\n",
      "[505/995] Writing tensor blk.16.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 338\n",
      "[506/995] Writing tensor blk.16.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 339\n",
      "[507/995] Writing tensor blk.16.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 340\n",
      "[508/995] Writing tensor blk.16.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 341\n",
      "[509/995] Writing tensor blk.16.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 342\n",
      "[510/995] Writing tensor blk.16.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 343\n",
      "[511/995] Writing tensor blk.16.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 344\n",
      "[512/995] Writing tensor blk.16.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 345\n",
      "[513/995] Writing tensor blk.16.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 346\n",
      "[514/995] Writing tensor blk.16.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 346\n",
      "[515/995] Writing tensor blk.16.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 347\n",
      "[516/995] Writing tensor blk.16.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 347\n",
      "[517/995] Writing tensor blk.16.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 348\n",
      "[518/995] Writing tensor blk.16.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 349\n",
      "[519/995] Writing tensor blk.16.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 349\n",
      "[520/995] Writing tensor blk.16.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 350\n",
      "[521/995] Writing tensor blk.16.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 350\n",
      "[522/995] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 350\n",
      "[523/995] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 350\n",
      "[524/995] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 350\n",
      "[525/995] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 351\n",
      "[526/995] Writing tensor blk.16.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 351\n",
      "[527/995] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+ 351\n",
      "[528/995] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+ 351\n",
      "[529/995] Writing tensor blk.17.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 354\n",
      "[530/995] Writing tensor blk.17.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 355\n",
      "[531/995] Writing tensor blk.17.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 356\n",
      "[532/995] Writing tensor blk.17.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 357\n",
      "[533/995] Writing tensor blk.17.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 357\n",
      "[534/995] Writing tensor blk.17.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 358\n",
      "[535/995] Writing tensor blk.17.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 358\n",
      "[536/995] Writing tensor blk.17.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 359\n",
      "[537/995] Writing tensor blk.17.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 360\n",
      "[538/995] Writing tensor blk.17.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 361\n",
      "[539/995] Writing tensor blk.17.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 362\n",
      "[540/995] Writing tensor blk.17.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 363\n",
      "[541/995] Writing tensor blk.17.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 364\n",
      "[542/995] Writing tensor blk.17.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 365\n",
      "[543/995] Writing tensor blk.17.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 366\n",
      "[544/995] Writing tensor blk.17.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 367\n",
      "[545/995] Writing tensor blk.17.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 367\n",
      "[546/995] Writing tensor blk.17.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 368\n",
      "[547/995] Writing tensor blk.17.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 369\n",
      "[548/995] Writing tensor blk.17.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 369\n",
      "[549/995] Writing tensor blk.17.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 370\n",
      "[550/995] Writing tensor blk.17.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 371\n",
      "[551/995] Writing tensor blk.17.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 372\n",
      "[552/995] Writing tensor blk.17.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 372\n",
      "[553/995] Writing tensor blk.17.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 373\n",
      "[554/995] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+ 373\n",
      "[555/995] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+ 373\n",
      "[556/995] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 373\n",
      "[557/995] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 373\n",
      "[558/995] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 373\n",
      "[559/995] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 373\n",
      "[560/995] Writing tensor blk.18.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 376\n",
      "[561/995] Writing tensor blk.18.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 378\n",
      "[562/995] Writing tensor blk.18.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 378\n",
      "[563/995] Writing tensor blk.18.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 379\n",
      "[564/995] Writing tensor blk.18.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 379\n",
      "[565/995] Writing tensor blk.18.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 379\n",
      "[566/995] Writing tensor blk.18.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 380\n",
      "[567/995] Writing tensor blk.18.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 381\n",
      "[568/995] Writing tensor blk.18.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 382\n",
      "[569/995] Writing tensor blk.18.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 383\n",
      "[570/995] Writing tensor blk.18.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 384\n",
      "[571/995] Writing tensor blk.18.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 384\n",
      "[572/995] Writing tensor blk.18.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 385\n",
      "[573/995] Writing tensor blk.18.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 385\n",
      "[574/995] Writing tensor blk.18.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 386\n",
      "[575/995] Writing tensor blk.18.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 387\n",
      "[576/995] Writing tensor blk.18.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 388\n",
      "[577/995] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 388\n",
      "[578/995] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 388\n",
      "[579/995] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 388\n",
      "[580/995] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 388\n",
      "[581/995] Writing tensor blk.18.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 388\n",
      "[582/995] Writing tensor blk.18.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 389\n",
      "[583/995] Writing tensor blk.18.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 390\n",
      "[584/995] Writing tensor blk.18.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 392\n",
      "[585/995] Writing tensor blk.18.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 393\n",
      "[586/995] Writing tensor blk.18.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 394\n",
      "[587/995] Writing tensor blk.18.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 395\n",
      "[588/995] Writing tensor blk.18.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 395\n",
      "[589/995] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+ 396\n",
      "[590/995] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+ 396\n",
      "[591/995] Writing tensor blk.19.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 396\n",
      "[592/995] Writing tensor blk.19.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 398\n",
      "[593/995] Writing tensor blk.19.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 399\n",
      "[594/995] Writing tensor blk.19.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 400\n",
      "[595/995] Writing tensor blk.19.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 401\n",
      "[596/995] Writing tensor blk.19.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 402\n",
      "[597/995] Writing tensor blk.19.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 403\n",
      "[598/995] Writing tensor blk.19.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 404\n",
      "[599/995] Writing tensor blk.19.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 405\n",
      "[600/995] Writing tensor blk.19.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 406\n",
      "[601/995] Writing tensor blk.19.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 406\n",
      "[602/995] Writing tensor blk.19.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 407\n",
      "[603/995] Writing tensor blk.19.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 408\n",
      "[604/995] Writing tensor blk.19.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 409\n",
      "[605/995] Writing tensor blk.19.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 410\n",
      "[606/995] Writing tensor blk.19.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 411\n",
      "[607/995] Writing tensor blk.19.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 412\n",
      "[608/995] Writing tensor blk.19.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 412\n",
      "[609/995] Writing tensor blk.19.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 413\n",
      "[610/995] Writing tensor blk.19.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 414\n",
      "[611/995] Writing tensor blk.19.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 415\n",
      "[612/995] Writing tensor blk.19.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 415\n",
      "[613/995] Writing tensor blk.19.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 416\n",
      "[614/995] Writing tensor blk.19.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 417\n",
      "[615/995] Writing tensor blk.19.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 417\n",
      "[616/995] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+ 417\n",
      "[617/995] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+ 417\n",
      "[618/995] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 417\n",
      "[619/995] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 417\n",
      "[620/995] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 417\n",
      "[621/995] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 417\n",
      "[622/995] Writing tensor blk.20.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 420\n",
      "[623/995] Writing tensor blk.20.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 422\n",
      "[624/995] Writing tensor blk.20.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 422\n",
      "[625/995] Writing tensor blk.20.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 422\n",
      "[626/995] Writing tensor blk.20.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 423\n",
      "[627/995] Writing tensor blk.20.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 424\n",
      "[628/995] Writing tensor blk.20.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 424\n",
      "[629/995] Writing tensor blk.20.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 425\n",
      "[630/995] Writing tensor blk.20.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 425\n",
      "[631/995] Writing tensor blk.20.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 426\n",
      "[632/995] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 426\n",
      "[633/995] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 426\n",
      "[634/995] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 426\n",
      "[635/995] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 426\n",
      "[636/995] Writing tensor blk.20.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 427\n",
      "[637/995] Writing tensor blk.20.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 428\n",
      "[638/995] Writing tensor blk.20.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 429\n",
      "[639/995] Writing tensor blk.20.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 431\n",
      "[640/995] Writing tensor blk.20.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 432\n",
      "[641/995] Writing tensor blk.20.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 432\n",
      "[642/995] Writing tensor blk.20.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 433\n",
      "[643/995] Writing tensor blk.20.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 434\n",
      "[644/995] Writing tensor blk.20.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 435\n",
      "[645/995] Writing tensor blk.20.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 436\n",
      "[646/995] Writing tensor blk.20.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 436\n",
      "[647/995] Writing tensor blk.20.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 437\n",
      "[648/995] Writing tensor blk.20.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 438\n",
      "[649/995] Writing tensor blk.20.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 439\n",
      "[650/995] Writing tensor blk.20.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 439\n",
      "[651/995] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+ 440\n",
      "[652/995] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+ 440\n",
      "[653/995] Writing tensor blk.21.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 440\n",
      "[654/995] Writing tensor blk.21.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 441\n",
      "[655/995] Writing tensor blk.21.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 442\n",
      "[656/995] Writing tensor blk.21.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 443\n",
      "[657/995] Writing tensor blk.21.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 445\n",
      "[658/995] Writing tensor blk.21.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 446\n",
      "[659/995] Writing tensor blk.21.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 447\n",
      "[660/995] Writing tensor blk.21.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 448\n",
      "[661/995] Writing tensor blk.21.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 449\n",
      "[662/995] Writing tensor blk.21.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 450\n",
      "[663/995] Writing tensor blk.21.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 451\n",
      "[664/995] Writing tensor blk.21.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 452\n",
      "[665/995] Writing tensor blk.21.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 452\n",
      "[666/995] Writing tensor blk.21.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 453\n",
      "[667/995] Writing tensor blk.21.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 454\n",
      "[668/995] Writing tensor blk.21.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 455\n",
      "[669/995] Writing tensor blk.21.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 456\n",
      "[670/995] Writing tensor blk.21.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 457\n",
      "[671/995] Writing tensor blk.21.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 458\n",
      "[672/995] Writing tensor blk.21.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 459\n",
      "[673/995] Writing tensor blk.21.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 459\n",
      "[674/995] Writing tensor blk.21.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 460\n",
      "[675/995] Writing tensor blk.21.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 460\n",
      "[676/995] Writing tensor blk.21.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 461\n",
      "[677/995] Writing tensor blk.21.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 461\n",
      "[678/995] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+ 461\n",
      "[679/995] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+ 461\n",
      "[680/995] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 461\n",
      "[681/995] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 461\n",
      "[682/995] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 462\n",
      "[683/995] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 462\n",
      "[684/995] Writing tensor blk.22.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 464\n",
      "[685/995] Writing tensor blk.22.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 464\n",
      "[686/995] Writing tensor blk.22.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 465\n",
      "[687/995] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 465\n",
      "[688/995] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 465\n",
      "[689/995] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 465\n",
      "[690/995] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 465\n",
      "[691/995] Writing tensor blk.22.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 465\n",
      "[692/995] Writing tensor blk.22.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 467\n",
      "[693/995] Writing tensor blk.22.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 469\n",
      "[694/995] Writing tensor blk.22.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 470\n",
      "[695/995] Writing tensor blk.22.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 470\n",
      "[696/995] Writing tensor blk.22.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 471\n",
      "[697/995] Writing tensor blk.22.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 471\n",
      "[698/995] Writing tensor blk.22.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 472\n",
      "[699/995] Writing tensor blk.22.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 473\n",
      "[700/995] Writing tensor blk.22.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 474\n",
      "[701/995] Writing tensor blk.22.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 475\n",
      "[702/995] Writing tensor blk.22.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 476\n",
      "[703/995] Writing tensor blk.22.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 477\n",
      "[704/995] Writing tensor blk.22.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 478\n",
      "[705/995] Writing tensor blk.22.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 479\n",
      "[706/995] Writing tensor blk.22.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 479\n",
      "[707/995] Writing tensor blk.22.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 480\n",
      "[708/995] Writing tensor blk.22.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 481\n",
      "[709/995] Writing tensor blk.22.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 482\n",
      "[710/995] Writing tensor blk.22.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 483\n",
      "[711/995] Writing tensor blk.22.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 484\n",
      "[712/995] Writing tensor blk.22.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 484\n",
      "[713/995] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+ 485\n",
      "[714/995] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 485\n",
      "[715/995] Writing tensor blk.23.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 485\n",
      "[716/995] Writing tensor blk.23.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 486\n",
      "[717/995] Writing tensor blk.23.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 487\n",
      "[718/995] Writing tensor blk.23.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 489\n",
      "[719/995] Writing tensor blk.23.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 490\n",
      "[720/995] Writing tensor blk.23.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 490\n",
      "[721/995] Writing tensor blk.23.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 491\n",
      "[722/995] Writing tensor blk.23.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 492\n",
      "[723/995] Writing tensor blk.23.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 493\n",
      "[724/995] Writing tensor blk.23.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 494\n",
      "[725/995] Writing tensor blk.23.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 495\n",
      "[726/995] Writing tensor blk.23.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 496\n",
      "[727/995] Writing tensor blk.23.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 497\n",
      "[728/995] Writing tensor blk.23.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 497\n",
      "[729/995] Writing tensor blk.23.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 498\n",
      "[730/995] Writing tensor blk.23.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 499\n",
      "[731/995] Writing tensor blk.23.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 500\n",
      "[732/995] Writing tensor blk.23.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 500\n",
      "[733/995] Writing tensor blk.23.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 501\n",
      "[734/995] Writing tensor blk.23.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 501\n",
      "[735/995] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 501\n",
      "[736/995] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 502\n",
      "[737/995] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 502\n",
      "[738/995] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 502\n",
      "[739/995] Writing tensor blk.23.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 502\n",
      "[740/995] Writing tensor blk.23.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 503\n",
      "[741/995] Writing tensor blk.23.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 504\n",
      "[742/995] Writing tensor blk.23.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 505\n",
      "[743/995] Writing tensor blk.23.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 505\n",
      "[744/995] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 506\n",
      "[745/995] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 506\n",
      "[746/995] Writing tensor blk.24.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 506\n",
      "[747/995] Writing tensor blk.24.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 507\n",
      "[748/995] Writing tensor blk.24.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 509\n",
      "[749/995] Writing tensor blk.24.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 510\n",
      "[750/995] Writing tensor blk.24.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 512\n",
      "[751/995] Writing tensor blk.24.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 513\n",
      "[752/995] Writing tensor blk.24.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 513\n",
      "[753/995] Writing tensor blk.24.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 514\n",
      "[754/995] Writing tensor blk.24.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 515\n",
      "[755/995] Writing tensor blk.24.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 516\n",
      "[756/995] Writing tensor blk.24.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 517\n",
      "[757/995] Writing tensor blk.24.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 518\n",
      "[758/995] Writing tensor blk.24.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 519\n",
      "[759/995] Writing tensor blk.24.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 520\n",
      "[760/995] Writing tensor blk.24.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 521\n",
      "[761/995] Writing tensor blk.24.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 522\n",
      "[762/995] Writing tensor blk.24.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 522\n",
      "[763/995] Writing tensor blk.24.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 523\n",
      "[764/995] Writing tensor blk.24.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 524\n",
      "[765/995] Writing tensor blk.24.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 525\n",
      "[766/995] Writing tensor blk.24.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 526\n",
      "[767/995] Writing tensor blk.24.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 526\n",
      "[768/995] Writing tensor blk.24.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 527\n",
      "[769/995] Writing tensor blk.24.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 527\n",
      "[770/995] Writing tensor blk.24.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 528\n",
      "[771/995] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 528\n",
      "[772/995] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 528\n",
      "[773/995] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 528\n",
      "[774/995] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 528\n",
      "[775/995] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 528\n",
      "[776/995] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 528\n",
      "[777/995] Writing tensor blk.25.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 531\n",
      "[778/995] Writing tensor blk.25.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 533\n",
      "[779/995] Writing tensor blk.25.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 533\n",
      "[780/995] Writing tensor blk.25.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 533\n",
      "[781/995] Writing tensor blk.25.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 534\n",
      "[782/995] Writing tensor blk.25.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 534\n",
      "[783/995] Writing tensor blk.25.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 535\n",
      "[784/995] Writing tensor blk.25.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 535\n",
      "[785/995] Writing tensor blk.25.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 536\n",
      "[786/995] Writing tensor blk.25.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 536\n",
      "[787/995] Writing tensor blk.25.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 537\n",
      "[788/995] Writing tensor blk.25.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 537\n",
      "[789/995] Writing tensor blk.25.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 538\n",
      "[790/995] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 538\n",
      "[791/995] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 538\n",
      "[792/995] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 539\n",
      "[793/995] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 539\n",
      "[794/995] Writing tensor blk.25.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 539\n",
      "[795/995] Writing tensor blk.25.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 540\n",
      "[796/995] Writing tensor blk.25.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 541\n",
      "[797/995] Writing tensor blk.25.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 543\n",
      "[798/995] Writing tensor blk.25.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 545\n",
      "[799/995] Writing tensor blk.25.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 546\n",
      "[800/995] Writing tensor blk.25.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 547\n",
      "[801/995] Writing tensor blk.25.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 547\n",
      "[802/995] Writing tensor blk.25.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 548\n",
      "[803/995] Writing tensor blk.25.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 549\n",
      "[804/995] Writing tensor blk.25.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 550\n",
      "[805/995] Writing tensor blk.25.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 551\n",
      "[806/995] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 552\n",
      "[807/995] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 552\n",
      "[808/995] Writing tensor blk.26.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 552\n",
      "[809/995] Writing tensor blk.26.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 553\n",
      "[810/995] Writing tensor blk.26.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 554\n",
      "[811/995] Writing tensor blk.26.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 555\n",
      "[812/995] Writing tensor blk.26.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 556\n",
      "[813/995] Writing tensor blk.26.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 557\n",
      "[814/995] Writing tensor blk.26.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 558\n",
      "[815/995] Writing tensor blk.26.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 559\n",
      "[816/995] Writing tensor blk.26.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 559\n",
      "[817/995] Writing tensor blk.26.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 560\n",
      "[818/995] Writing tensor blk.26.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 561\n",
      "[819/995] Writing tensor blk.26.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 562\n",
      "[820/995] Writing tensor blk.26.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 563\n",
      "[821/995] Writing tensor blk.26.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 564\n",
      "[822/995] Writing tensor blk.26.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 565\n",
      "[823/995] Writing tensor blk.26.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 566\n",
      "[824/995] Writing tensor blk.26.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 567\n",
      "[825/995] Writing tensor blk.26.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 567\n",
      "[826/995] Writing tensor blk.26.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 568\n",
      "[827/995] Writing tensor blk.26.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 569\n",
      "[828/995] Writing tensor blk.26.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 569\n",
      "[829/995] Writing tensor blk.26.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 570\n",
      "[830/995] Writing tensor blk.26.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 571\n",
      "[831/995] Writing tensor blk.26.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 571\n",
      "[832/995] Writing tensor blk.26.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 572\n",
      "[833/995] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 572\n",
      "[834/995] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 572\n",
      "[835/995] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 572\n",
      "[836/995] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 572\n",
      "[837/995] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 572\n",
      "[838/995] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 572\n",
      "[839/995] Writing tensor blk.27.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 575\n",
      "[840/995] Writing tensor blk.27.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 575\n",
      "[841/995] Writing tensor blk.27.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 576\n",
      "[842/995] Writing tensor blk.27.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 576\n",
      "[843/995] Writing tensor blk.27.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 577\n",
      "[844/995] Writing tensor blk.27.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 577\n",
      "[845/995] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 577\n",
      "[846/995] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 577\n",
      "[847/995] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 578\n",
      "[848/995] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 578\n",
      "[849/995] Writing tensor blk.27.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 579\n",
      "[850/995] Writing tensor blk.27.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 580\n",
      "[851/995] Writing tensor blk.27.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 580\n",
      "[852/995] Writing tensor blk.27.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 582\n",
      "[853/995] Writing tensor blk.27.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 582\n",
      "[854/995] Writing tensor blk.27.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 583\n",
      "[855/995] Writing tensor blk.27.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 584\n",
      "[856/995] Writing tensor blk.27.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 586\n",
      "[857/995] Writing tensor blk.27.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 587\n",
      "[858/995] Writing tensor blk.27.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 588\n",
      "[859/995] Writing tensor blk.27.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 588\n",
      "[860/995] Writing tensor blk.27.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 589\n",
      "[861/995] Writing tensor blk.27.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 590\n",
      "[862/995] Writing tensor blk.27.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 591\n",
      "[863/995] Writing tensor blk.27.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 592\n",
      "[864/995] Writing tensor blk.27.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 593\n",
      "[865/995] Writing tensor blk.27.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 594\n",
      "[866/995] Writing tensor blk.27.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 595\n",
      "[867/995] Writing tensor blk.27.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 595\n",
      "[868/995] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 596\n",
      "[869/995] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 596\n",
      "[870/995] Writing tensor blk.28.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 596\n",
      "[871/995] Writing tensor blk.28.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 597\n",
      "[872/995] Writing tensor blk.28.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 598\n",
      "[873/995] Writing tensor blk.28.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 599\n",
      "[874/995] Writing tensor blk.28.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 600\n",
      "[875/995] Writing tensor blk.28.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 601\n",
      "[876/995] Writing tensor blk.28.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 602\n",
      "[877/995] Writing tensor blk.28.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 603\n",
      "[878/995] Writing tensor blk.28.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 603\n",
      "[879/995] Writing tensor blk.28.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 605\n",
      "[880/995] Writing tensor blk.28.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 605\n",
      "[881/995] Writing tensor blk.28.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 606\n",
      "[882/995] Writing tensor blk.28.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 607\n",
      "[883/995] Writing tensor blk.28.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 608\n",
      "[884/995] Writing tensor blk.28.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 609\n",
      "[885/995] Writing tensor blk.28.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 610\n",
      "[886/995] Writing tensor blk.28.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 610\n",
      "[887/995] Writing tensor blk.28.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 611\n",
      "[888/995] Writing tensor blk.28.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 612\n",
      "[889/995] Writing tensor blk.28.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 613\n",
      "[890/995] Writing tensor blk.28.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 613\n",
      "[891/995] Writing tensor blk.28.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 614\n",
      "[892/995] Writing tensor blk.28.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 615\n",
      "[893/995] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 615\n",
      "[894/995] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 615\n",
      "[895/995] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 615\n",
      "[896/995] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 615\n",
      "[897/995] Writing tensor blk.28.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 615\n",
      "[898/995] Writing tensor blk.28.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 616\n",
      "[899/995] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 617\n",
      "[900/995] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 617\n",
      "[901/995] Writing tensor blk.29.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 619\n",
      "[902/995] Writing tensor blk.29.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 619\n",
      "[903/995] Writing tensor blk.29.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 620\n",
      "[904/995] Writing tensor blk.29.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 621\n",
      "[905/995] Writing tensor blk.29.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 622\n",
      "[906/995] Writing tensor blk.29.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 623\n",
      "[907/995] Writing tensor blk.29.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 624\n",
      "[908/995] Writing tensor blk.29.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 625\n",
      "[909/995] Writing tensor blk.29.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 626\n",
      "[910/995] Writing tensor blk.29.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 627\n",
      "[911/995] Writing tensor blk.29.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 628\n",
      "[912/995] Writing tensor blk.29.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 628\n",
      "[913/995] Writing tensor blk.29.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 629\n",
      "[914/995] Writing tensor blk.29.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 630\n",
      "[915/995] Writing tensor blk.29.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 631\n",
      "[916/995] Writing tensor blk.29.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 632\n",
      "[917/995] Writing tensor blk.29.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 633\n",
      "[918/995] Writing tensor blk.29.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 634\n",
      "[919/995] Writing tensor blk.29.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 634\n",
      "[920/995] Writing tensor blk.29.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 635\n",
      "[921/995] Writing tensor blk.29.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 636\n",
      "[922/995] Writing tensor blk.29.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 636\n",
      "[923/995] Writing tensor blk.29.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 637\n",
      "[924/995] Writing tensor blk.29.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 638\n",
      "[925/995] Writing tensor blk.29.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 638\n",
      "[926/995] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 638\n",
      "[927/995] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 638\n",
      "[928/995] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 638\n",
      "[929/995] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 638\n",
      "[930/995] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 638\n",
      "[931/995] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 638\n",
      "[932/995] Writing tensor blk.30.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 642\n",
      "[933/995] Writing tensor blk.30.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 643\n",
      "[934/995] Writing tensor blk.30.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 643\n",
      "[935/995] Writing tensor blk.30.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 643\n",
      "[936/995] Writing tensor blk.30.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 644\n",
      "[937/995] Writing tensor blk.30.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 644\n",
      "[938/995] Writing tensor blk.30.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 645\n",
      "[939/995] Writing tensor blk.30.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 646\n",
      "[940/995] Writing tensor blk.30.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 647\n",
      "[941/995] Writing tensor blk.30.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 648\n",
      "[942/995] Writing tensor blk.30.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 649\n",
      "[943/995] Writing tensor blk.30.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 649\n",
      "[944/995] Writing tensor blk.30.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 650\n",
      "[945/995] Writing tensor blk.30.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 651\n",
      "[946/995] Writing tensor blk.30.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 652\n",
      "[947/995] Writing tensor blk.30.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 652\n",
      "[948/995] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 652\n",
      "[949/995] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 653\n",
      "[950/995] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 653\n",
      "[951/995] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 653\n",
      "[952/995] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+ 657\n",
      "[953/995] Writing tensor blk.30.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 658\n",
      "[954/995] Writing tensor blk.30.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 659\n",
      "[955/995] Writing tensor blk.30.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 659\n",
      "[956/995] Writing tensor blk.30.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 660\n",
      "[957/995] Writing tensor blk.30.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 661\n",
      "[958/995] Writing tensor blk.30.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 661\n",
      "[959/995] Writing tensor blk.30.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 662\n",
      "[960/995] Writing tensor blk.30.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 663\n",
      "[961/995] Writing tensor blk.30.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 664\n",
      "[962/995] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 664\n",
      "[963/995] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 664\n",
      "[964/995] Writing tensor blk.31.ffn_gate.0.weight               | size  14336 x   4096  | type F16  | T+ 664\n",
      "[965/995] Writing tensor blk.31.ffn_down.0.weight               | size   4096 x  14336  | type F16  | T+ 665\n",
      "[966/995] Writing tensor blk.31.ffn_up.0.weight                 | size  14336 x   4096  | type F16  | T+ 666\n",
      "[967/995] Writing tensor blk.31.ffn_gate.1.weight               | size  14336 x   4096  | type F16  | T+ 667\n",
      "[968/995] Writing tensor blk.31.ffn_down.1.weight               | size   4096 x  14336  | type F16  | T+ 668\n",
      "[969/995] Writing tensor blk.31.ffn_up.1.weight                 | size  14336 x   4096  | type F16  | T+ 669\n",
      "[970/995] Writing tensor blk.31.ffn_gate.2.weight               | size  14336 x   4096  | type F16  | T+ 670\n",
      "[971/995] Writing tensor blk.31.ffn_down.2.weight               | size   4096 x  14336  | type F16  | T+ 671\n",
      "[972/995] Writing tensor blk.31.ffn_up.2.weight                 | size  14336 x   4096  | type F16  | T+ 672\n",
      "[973/995] Writing tensor blk.31.ffn_gate.3.weight               | size  14336 x   4096  | type F16  | T+ 673\n",
      "[974/995] Writing tensor blk.31.ffn_down.3.weight               | size   4096 x  14336  | type F16  | T+ 674\n",
      "[975/995] Writing tensor blk.31.ffn_up.3.weight                 | size  14336 x   4096  | type F16  | T+ 675\n",
      "[976/995] Writing tensor blk.31.ffn_gate.4.weight               | size  14336 x   4096  | type F16  | T+ 676\n",
      "[977/995] Writing tensor blk.31.ffn_down.4.weight               | size   4096 x  14336  | type F16  | T+ 677\n",
      "[978/995] Writing tensor blk.31.ffn_up.4.weight                 | size  14336 x   4096  | type F16  | T+ 677\n",
      "[979/995] Writing tensor blk.31.ffn_gate.5.weight               | size  14336 x   4096  | type F16  | T+ 678\n",
      "[980/995] Writing tensor blk.31.ffn_down.5.weight               | size   4096 x  14336  | type F16  | T+ 679\n",
      "[981/995] Writing tensor blk.31.ffn_up.5.weight                 | size  14336 x   4096  | type F16  | T+ 680\n",
      "[982/995] Writing tensor blk.31.ffn_gate.6.weight               | size  14336 x   4096  | type F16  | T+ 681\n",
      "[983/995] Writing tensor blk.31.ffn_down.6.weight               | size   4096 x  14336  | type F16  | T+ 681\n",
      "[984/995] Writing tensor blk.31.ffn_up.6.weight                 | size  14336 x   4096  | type F16  | T+ 682\n",
      "[985/995] Writing tensor blk.31.ffn_gate.7.weight               | size  14336 x   4096  | type F16  | T+ 683\n",
      "[986/995] Writing tensor blk.31.ffn_down.7.weight               | size   4096 x  14336  | type F16  | T+ 683\n",
      "[987/995] Writing tensor blk.31.ffn_up.7.weight                 | size  14336 x   4096  | type F16  | T+ 684\n",
      "[988/995] Writing tensor blk.31.ffn_gate_inp.weight             | size      8 x   4096  | type F16  | T+ 684\n",
      "[989/995] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 684\n",
      "[990/995] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 684\n",
      "[991/995] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 684\n",
      "[992/995] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 684\n",
      "[993/995] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 685\n",
      "[994/995] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 685\n",
      "[995/995] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 685\n",
      "Wrote mixtral-8x7b-instruct-v0.1.fp16.bin\n"
     ]
    }
   ],
   "source": [
    "# Convert to fp16\n",
    "model_folder = f\"../{MODEL_NAME}/\"\n",
    "fp16 = f\"{MODEL_NAME.lower()}.fp16.bin\"\n",
    "!python3 /home/ubuntu/llama.cpp/convert.py {model_folder} --outtype f16 --outfile {fp16}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eca026f6-0910-4c95-a01f-ef0aaeb45ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
      "main: build = 1706 (b47879b)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing 'mixtral-8x7b-instruct-v0.1.fp16.bin' to '/home/ubuntu/Mixtral-8x7B-Instruct-v0.1-GGUF/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf' as Q4_K_M\n",
      "llama_model_loader: loaded meta data with 25 key-value pairs and 995 tensors from mixtral-8x7b-instruct-v0.1.fp16.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = ..\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,58980]   = [\" t\", \"i n\", \"e r\", \" a\", \"h e...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  930 tensors\n",
      "llama_model_quantize_internal ============ Strange model: n_attention_wv = 32, n_feed_forward_w2 = 0, hparams.n_layer = 32\n",
      "llama_model_quantize_internal: meta size = 1716480 bytes\n",
      "[   1/ 995]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_K .. size =   250.00 MiB ->    70.31 MiB | hist: \n",
      "[   2/ 995]              blk.0.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[   3/ 995]              blk.0.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[   4/ 995]                blk.0.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[   5/ 995]              blk.0.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[   6/ 995]              blk.0.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[   7/ 995]                blk.0.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[   8/ 995]              blk.0.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[   9/ 995]              blk.0.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  10/ 995]                blk.0.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  11/ 995]              blk.0.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  12/ 995]              blk.0.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  13/ 995]                blk.0.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  14/ 995]              blk.0.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  15/ 995]              blk.0.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  16/ 995]                blk.0.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  17/ 995]              blk.0.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  18/ 995]              blk.0.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  19/ 995]                blk.0.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  20/ 995]              blk.0.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  21/ 995]              blk.0.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  22/ 995]                blk.0.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  23/ 995]              blk.0.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  24/ 995]              blk.0.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  25/ 995]                blk.0.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  26/ 995]            blk.0.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[  27/ 995]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  28/ 995]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  29/ 995]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.022 0.010 0.015 0.021 0.030 0.041 0.061 0.599 0.061 0.041 0.030 0.021 0.015 0.010 0.022 \n",
      "[  30/ 995]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[  31/ 995]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[  32/ 995]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.023 0.012 0.019 0.028 0.042 0.066 0.113 0.392 0.113 0.066 0.042 0.028 0.019 0.012 0.024 \n",
      "[  33/ 995]              blk.1.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  34/ 995]              blk.1.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  35/ 995]                blk.1.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  36/ 995]              blk.1.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  37/ 995]              blk.1.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  38/ 995]                blk.1.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  39/ 995]              blk.1.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  40/ 995]              blk.1.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  41/ 995]                blk.1.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  42/ 995]              blk.1.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  43/ 995]              blk.1.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  44/ 995]                blk.1.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  45/ 995]              blk.1.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  46/ 995]              blk.1.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  47/ 995]            blk.1.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[  48/ 995]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.024 0.014 0.022 0.033 0.046 0.064 0.082 0.430 0.082 0.064 0.046 0.033 0.022 0.014 0.024 \n",
      "[  49/ 995]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[  50/ 995]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[  51/ 995]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.025 0.015 0.023 0.034 0.047 0.063 0.081 0.423 0.081 0.063 0.047 0.034 0.023 0.015 0.025 \n",
      "[  52/ 995]                blk.1.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  53/ 995]              blk.1.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  54/ 995]              blk.1.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  55/ 995]                blk.1.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  56/ 995]              blk.1.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  57/ 995]              blk.1.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  58/ 995]                blk.1.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  59/ 995]              blk.1.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  60/ 995]              blk.1.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  61/ 995]                blk.1.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  62/ 995]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  63/ 995]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  64/ 995]              blk.2.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  65/ 995]              blk.2.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  66/ 995]                blk.2.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  67/ 995]              blk.2.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  68/ 995]              blk.2.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  69/ 995]                blk.2.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  70/ 995]              blk.2.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  71/ 995]              blk.2.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  72/ 995]                blk.2.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  73/ 995]              blk.2.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  74/ 995]              blk.2.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  75/ 995]                blk.2.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  76/ 995]              blk.2.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  77/ 995]              blk.2.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  78/ 995]                blk.2.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  79/ 995]              blk.2.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  80/ 995]              blk.2.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  81/ 995]                blk.2.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  82/ 995]              blk.2.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  83/ 995]              blk.2.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  84/ 995]                blk.2.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  85/ 995]              blk.2.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  86/ 995]              blk.2.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  87/ 995]                blk.2.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  88/ 995]            blk.2.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[  89/ 995]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  90/ 995]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  91/ 995]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.018 0.029 0.045 0.064 0.087 0.109 0.242 0.109 0.087 0.064 0.045 0.029 0.018 0.026 \n",
      "[  92/ 995]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[  93/ 995]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[  94/ 995]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.028 0.043 0.062 0.085 0.110 0.254 0.110 0.085 0.062 0.043 0.028 0.018 0.026 \n",
      "[  95/ 995]              blk.3.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  96/ 995]              blk.3.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  97/ 995]                blk.3.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  98/ 995]              blk.3.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[  99/ 995]              blk.3.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 100/ 995]                blk.3.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 101/ 995]              blk.3.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 102/ 995]            blk.3.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 103/ 995]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.107 0.233 0.107 0.088 0.066 0.046 0.030 0.019 0.027 \n",
      "[ 104/ 995]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 105/ 995]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 106/ 995]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.237 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 107/ 995]              blk.3.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 108/ 995]                blk.3.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 109/ 995]              blk.3.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 110/ 995]              blk.3.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 111/ 995]                blk.3.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 112/ 995]              blk.3.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 113/ 995]              blk.3.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 114/ 995]                blk.3.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 115/ 995]              blk.3.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 116/ 995]              blk.3.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 117/ 995]                blk.3.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 118/ 995]              blk.3.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 119/ 995]              blk.3.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 120/ 995]                blk.3.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 121/ 995]              blk.3.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 122/ 995]              blk.3.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 123/ 995]                blk.3.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 124/ 995]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 125/ 995]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 126/ 995]              blk.4.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 127/ 995]              blk.4.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 128/ 995]                blk.4.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 129/ 995]              blk.4.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 130/ 995]              blk.4.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 131/ 995]                blk.4.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 132/ 995]              blk.4.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 133/ 995]              blk.4.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 134/ 995]                blk.4.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 135/ 995]              blk.4.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 136/ 995]              blk.4.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 137/ 995]                blk.4.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 138/ 995]              blk.4.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 139/ 995]              blk.4.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 140/ 995]                blk.4.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 141/ 995]              blk.4.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 142/ 995]              blk.4.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 143/ 995]                blk.4.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 144/ 995]              blk.4.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 145/ 995]              blk.4.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 146/ 995]                blk.4.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 147/ 995]              blk.4.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 148/ 995]              blk.4.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 149/ 995]                blk.4.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 150/ 995]            blk.4.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 151/ 995]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 152/ 995]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 153/ 995]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.232 0.107 0.087 0.066 0.047 0.031 0.019 0.027 \n",
      "[ 154/ 995]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 155/ 995]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 156/ 995]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.238 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 157/ 995]            blk.5.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 158/ 995]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.233 0.107 0.088 0.066 0.046 0.030 0.019 0.027 \n",
      "[ 159/ 995]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 160/ 995]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 161/ 995]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.239 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 162/ 995]              blk.5.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 163/ 995]              blk.5.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 164/ 995]                blk.5.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 165/ 995]              blk.5.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 166/ 995]              blk.5.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 167/ 995]                blk.5.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 168/ 995]              blk.5.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 169/ 995]              blk.5.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 170/ 995]                blk.5.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 171/ 995]              blk.5.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 172/ 995]              blk.5.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 173/ 995]                blk.5.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 174/ 995]              blk.5.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 175/ 995]              blk.5.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 176/ 995]                blk.5.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 177/ 995]              blk.5.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 178/ 995]              blk.5.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 179/ 995]                blk.5.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 180/ 995]              blk.5.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 181/ 995]              blk.5.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 182/ 995]                blk.5.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 183/ 995]              blk.5.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 184/ 995]              blk.5.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 185/ 995]                blk.5.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 186/ 995]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 187/ 995]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 188/ 995]              blk.6.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 189/ 995]              blk.6.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 190/ 995]                blk.6.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 191/ 995]              blk.6.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 192/ 995]              blk.6.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 193/ 995]                blk.6.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 194/ 995]              blk.6.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 195/ 995]              blk.6.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 196/ 995]                blk.6.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 197/ 995]              blk.6.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 198/ 995]              blk.6.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 199/ 995]                blk.6.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 200/ 995]              blk.6.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 201/ 995]              blk.6.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 202/ 995]                blk.6.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 203/ 995]              blk.6.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 204/ 995]              blk.6.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 205/ 995]            blk.6.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 206/ 995]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.233 0.107 0.088 0.066 0.046 0.030 0.019 0.027 \n",
      "[ 207/ 995]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 208/ 995]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 209/ 995]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.064 0.087 0.108 0.240 0.108 0.087 0.064 0.046 0.030 0.019 0.027 \n",
      "[ 210/ 995]                blk.6.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 211/ 995]              blk.6.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 212/ 995]              blk.6.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 213/ 995]                blk.6.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 214/ 995]              blk.6.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 215/ 995]              blk.6.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 216/ 995]                blk.6.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 217/ 995]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 218/ 995]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 219/ 995]              blk.7.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 220/ 995]              blk.7.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 221/ 995]                blk.7.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 222/ 995]              blk.7.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 223/ 995]              blk.7.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 224/ 995]                blk.7.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 225/ 995]              blk.7.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 226/ 995]              blk.7.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 227/ 995]                blk.7.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 228/ 995]              blk.7.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 229/ 995]              blk.7.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 230/ 995]                blk.7.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 231/ 995]              blk.7.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 232/ 995]              blk.7.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 233/ 995]                blk.7.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 234/ 995]              blk.7.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 235/ 995]              blk.7.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 236/ 995]                blk.7.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 237/ 995]              blk.7.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 238/ 995]              blk.7.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 239/ 995]                blk.7.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 240/ 995]              blk.7.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 241/ 995]              blk.7.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 242/ 995]                blk.7.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 243/ 995]            blk.7.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 244/ 995]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 245/ 995]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 246/ 995]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 247/ 995]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 248/ 995]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 249/ 995]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.063 0.085 0.107 0.248 0.107 0.086 0.064 0.045 0.030 0.019 0.027 \n",
      "[ 250/ 995]              blk.8.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 251/ 995]              blk.8.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 252/ 995]                blk.8.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 253/ 995]              blk.8.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 254/ 995]              blk.8.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 255/ 995]                blk.8.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 256/ 995]              blk.8.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 257/ 995]              blk.8.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 258/ 995]                blk.8.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 259/ 995]              blk.8.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 260/ 995]            blk.8.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 261/ 995]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.026 \n",
      "[ 262/ 995]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 263/ 995]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 264/ 995]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.064 0.086 0.108 0.241 0.108 0.086 0.064 0.045 0.030 0.019 0.027 \n",
      "[ 265/ 995]             blk.10.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 266/ 995]             blk.10.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 267/ 995]               blk.10.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 268/ 995]           blk.10.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 269/ 995]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.030 0.045 0.065 0.088 0.109 0.238 0.109 0.088 0.065 0.045 0.029 0.018 0.026 \n",
      "[ 270/ 995]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 271/ 995]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 272/ 995]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.029 0.045 0.064 0.087 0.108 0.242 0.108 0.087 0.064 0.045 0.030 0.019 0.027 \n",
      "[ 273/ 995]              blk.8.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 274/ 995]                blk.8.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 275/ 995]              blk.8.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 276/ 995]              blk.8.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 277/ 995]                blk.8.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 278/ 995]              blk.8.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 279/ 995]              blk.8.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 280/ 995]                blk.8.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 281/ 995]              blk.8.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 282/ 995]              blk.8.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 283/ 995]                blk.8.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 284/ 995]              blk.8.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 285/ 995]              blk.8.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 286/ 995]                blk.8.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 287/ 995]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 288/ 995]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 289/ 995]              blk.9.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 290/ 995]              blk.9.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 291/ 995]                blk.9.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 292/ 995]              blk.9.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 293/ 995]              blk.9.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 294/ 995]                blk.9.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 295/ 995]              blk.9.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 296/ 995]              blk.9.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 297/ 995]                blk.9.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 298/ 995]              blk.9.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 299/ 995]              blk.9.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 300/ 995]                blk.9.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 301/ 995]              blk.9.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 302/ 995]              blk.9.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 303/ 995]                blk.9.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 304/ 995]              blk.9.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 305/ 995]              blk.9.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 306/ 995]                blk.9.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 307/ 995]              blk.9.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 308/ 995]              blk.9.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 309/ 995]                blk.9.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 310/ 995]              blk.9.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 311/ 995]              blk.9.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 312/ 995]                blk.9.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 313/ 995]            blk.9.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 314/ 995]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 315/ 995]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 316/ 995]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 317/ 995]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 318/ 995]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 319/ 995]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.029 0.045 0.064 0.086 0.108 0.245 0.108 0.086 0.063 0.045 0.029 0.019 0.027 \n",
      "[ 320/ 995]             blk.10.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 321/ 995]             blk.10.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 322/ 995]               blk.10.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 323/ 995]             blk.10.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 324/ 995]             blk.10.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 325/ 995]               blk.10.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 326/ 995]             blk.10.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 327/ 995]             blk.10.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 328/ 995]               blk.10.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 329/ 995]             blk.10.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 330/ 995]             blk.10.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 331/ 995]               blk.10.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 332/ 995]             blk.10.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 333/ 995]             blk.10.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 334/ 995]               blk.10.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 335/ 995]             blk.10.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 336/ 995]             blk.10.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 337/ 995]               blk.10.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 338/ 995]             blk.10.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 339/ 995]             blk.10.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 340/ 995]               blk.10.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 341/ 995]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 342/ 995]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 343/ 995]             blk.11.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 344/ 995]             blk.11.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 345/ 995]               blk.11.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 346/ 995]             blk.11.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 347/ 995]             blk.11.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 348/ 995]               blk.11.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 349/ 995]             blk.11.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 350/ 995]             blk.11.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 351/ 995]               blk.11.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 352/ 995]             blk.11.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 353/ 995]             blk.11.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 354/ 995]               blk.11.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 355/ 995]             blk.11.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 356/ 995]             blk.11.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 357/ 995]               blk.11.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 358/ 995]             blk.11.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 359/ 995]             blk.11.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 360/ 995]               blk.11.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 361/ 995]             blk.11.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 362/ 995]             blk.11.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 363/ 995]           blk.11.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 364/ 995]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.109 0.236 0.109 0.088 0.065 0.046 0.030 0.018 0.026 \n",
      "[ 365/ 995]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 366/ 995]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 367/ 995]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.064 0.087 0.108 0.241 0.108 0.087 0.064 0.045 0.030 0.019 0.027 \n",
      "[ 368/ 995]               blk.11.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 369/ 995]             blk.11.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 370/ 995]             blk.11.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 371/ 995]               blk.11.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 372/ 995]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 373/ 995]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 374/ 995]             blk.12.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 375/ 995]             blk.12.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 376/ 995]               blk.12.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 377/ 995]             blk.12.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 378/ 995]             blk.12.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 379/ 995]               blk.12.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 380/ 995]             blk.12.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 381/ 995]             blk.12.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 382/ 995]               blk.12.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 383/ 995]             blk.12.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 384/ 995]             blk.12.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 385/ 995]               blk.12.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 386/ 995]             blk.12.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 387/ 995]             blk.12.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 388/ 995]               blk.12.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 389/ 995]             blk.12.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 390/ 995]             blk.12.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 391/ 995]               blk.12.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 392/ 995]             blk.12.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 393/ 995]             blk.12.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 394/ 995]               blk.12.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 395/ 995]             blk.12.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 396/ 995]             blk.12.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 397/ 995]               blk.12.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 398/ 995]           blk.12.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 399/ 995]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 400/ 995]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 401/ 995]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.088 0.109 0.239 0.109 0.088 0.064 0.045 0.029 0.018 0.026 \n",
      "[ 402/ 995]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 403/ 995]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 404/ 995]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.063 0.086 0.110 0.249 0.109 0.086 0.063 0.044 0.029 0.018 0.026 \n",
      "[ 405/ 995]             blk.13.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 406/ 995]             blk.13.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 407/ 995]               blk.13.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 408/ 995]             blk.13.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 409/ 995]             blk.13.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 410/ 995]               blk.13.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 411/ 995]             blk.13.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 412/ 995]             blk.13.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 413/ 995]               blk.13.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 414/ 995]             blk.13.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 415/ 995]             blk.13.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 416/ 995]               blk.13.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 417/ 995]             blk.13.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 418/ 995]           blk.13.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 419/ 995]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.109 0.237 0.109 0.088 0.065 0.046 0.030 0.018 0.027 \n",
      "[ 420/ 995]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 421/ 995]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 422/ 995]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.029 0.045 0.064 0.087 0.109 0.242 0.108 0.087 0.064 0.045 0.029 0.018 0.027 \n",
      "[ 423/ 995]             blk.13.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 424/ 995]               blk.13.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 425/ 995]             blk.13.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 426/ 995]             blk.13.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 427/ 995]               blk.13.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 428/ 995]             blk.13.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 429/ 995]             blk.13.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 430/ 995]               blk.13.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 431/ 995]             blk.13.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 432/ 995]             blk.13.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 433/ 995]               blk.13.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 434/ 995]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 435/ 995]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 436/ 995]             blk.14.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 437/ 995]             blk.14.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 438/ 995]               blk.14.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 439/ 995]             blk.14.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 440/ 995]             blk.14.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 441/ 995]               blk.14.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 442/ 995]             blk.14.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 443/ 995]             blk.14.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 444/ 995]               blk.14.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 445/ 995]             blk.14.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 446/ 995]             blk.14.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 447/ 995]               blk.14.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 448/ 995]             blk.14.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 449/ 995]             blk.14.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 450/ 995]               blk.14.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 451/ 995]             blk.14.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 452/ 995]             blk.14.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 453/ 995]               blk.14.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 454/ 995]             blk.14.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 455/ 995]             blk.14.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 456/ 995]               blk.14.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 457/ 995]             blk.14.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 458/ 995]             blk.14.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 459/ 995]               blk.14.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 460/ 995]           blk.14.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 461/ 995]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 462/ 995]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 463/ 995]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.088 0.109 0.239 0.110 0.088 0.064 0.045 0.029 0.018 0.026 \n",
      "[ 464/ 995]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 465/ 995]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 466/ 995]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.063 0.086 0.109 0.247 0.108 0.086 0.063 0.044 0.029 0.018 0.026 \n",
      "[ 467/ 995]             blk.15.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 468/ 995]             blk.15.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 469/ 995]               blk.15.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 470/ 995]             blk.15.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 471/ 995]             blk.15.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 472/ 995]               blk.15.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 473/ 995]           blk.15.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 474/ 995]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.087 0.109 0.241 0.109 0.087 0.064 0.045 0.029 0.018 0.026 \n",
      "[ 475/ 995]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 476/ 995]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 477/ 995]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.063 0.086 0.109 0.247 0.108 0.087 0.063 0.044 0.029 0.018 0.026 \n",
      "[ 478/ 995]             blk.15.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 479/ 995]             blk.15.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 480/ 995]               blk.15.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 481/ 995]             blk.15.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 482/ 995]             blk.15.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 483/ 995]               blk.15.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 484/ 995]             blk.15.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 485/ 995]             blk.15.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 486/ 995]               blk.15.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 487/ 995]             blk.15.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 488/ 995]             blk.15.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 489/ 995]               blk.15.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 490/ 995]             blk.15.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 491/ 995]             blk.15.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 492/ 995]               blk.15.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 493/ 995]             blk.15.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 494/ 995]             blk.15.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 495/ 995]               blk.15.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 496/ 995]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 497/ 995]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 498/ 995]             blk.16.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 499/ 995]             blk.16.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 500/ 995]               blk.16.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 501/ 995]             blk.16.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 502/ 995]             blk.16.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 503/ 995]               blk.16.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 504/ 995]             blk.16.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 505/ 995]             blk.16.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 506/ 995]               blk.16.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 507/ 995]             blk.16.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 508/ 995]             blk.16.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 509/ 995]               blk.16.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 510/ 995]             blk.16.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 511/ 995]             blk.16.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 512/ 995]               blk.16.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 513/ 995]             blk.16.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 514/ 995]             blk.16.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 515/ 995]               blk.16.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 516/ 995]             blk.16.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 517/ 995]             blk.16.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 518/ 995]               blk.16.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 519/ 995]             blk.16.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 520/ 995]             blk.16.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 521/ 995]           blk.16.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 522/ 995]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.018 0.029 0.045 0.065 0.088 0.109 0.237 0.109 0.088 0.065 0.046 0.029 0.018 0.026 \n",
      "[ 523/ 995]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 524/ 995]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 525/ 995]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.063 0.086 0.109 0.248 0.109 0.087 0.063 0.044 0.029 0.018 0.026 \n",
      "[ 526/ 995]               blk.16.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 527/ 995]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 528/ 995]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 529/ 995]             blk.17.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 530/ 995]             blk.17.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 531/ 995]               blk.17.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 532/ 995]             blk.17.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 533/ 995]             blk.17.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 534/ 995]               blk.17.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 535/ 995]             blk.17.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 536/ 995]             blk.17.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 537/ 995]               blk.17.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 538/ 995]             blk.17.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 539/ 995]             blk.17.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 540/ 995]               blk.17.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 541/ 995]             blk.17.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 542/ 995]             blk.17.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 543/ 995]               blk.17.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 544/ 995]             blk.17.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 545/ 995]             blk.17.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 546/ 995]               blk.17.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 547/ 995]             blk.17.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 548/ 995]             blk.17.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 549/ 995]               blk.17.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 550/ 995]             blk.17.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 551/ 995]             blk.17.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 552/ 995]               blk.17.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 553/ 995]           blk.17.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 554/ 995]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 555/ 995]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 556/ 995]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.065 0.088 0.109 0.238 0.109 0.088 0.065 0.046 0.030 0.018 0.027 \n",
      "[ 557/ 995]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 558/ 995]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 559/ 995]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.063 0.087 0.110 0.247 0.110 0.087 0.063 0.044 0.029 0.018 0.026 \n",
      "[ 560/ 995]             blk.18.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 561/ 995]             blk.18.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 562/ 995]               blk.18.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 563/ 995]             blk.18.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 564/ 995]             blk.18.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 565/ 995]               blk.18.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 566/ 995]             blk.18.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 567/ 995]             blk.18.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 568/ 995]               blk.18.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 569/ 995]             blk.18.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 570/ 995]             blk.18.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 571/ 995]               blk.18.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 572/ 995]             blk.18.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 573/ 995]             blk.18.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 574/ 995]               blk.18.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 575/ 995]             blk.18.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 576/ 995]           blk.18.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 577/ 995]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.234 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 578/ 995]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 579/ 995]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 580/ 995]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.028 0.044 0.063 0.086 0.110 0.249 0.110 0.086 0.063 0.044 0.029 0.018 0.026 \n",
      "[ 581/ 995]             blk.18.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 582/ 995]               blk.18.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 583/ 995]             blk.18.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 584/ 995]             blk.18.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 585/ 995]               blk.18.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 586/ 995]             blk.18.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 587/ 995]             blk.18.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 588/ 995]               blk.18.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 589/ 995]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 590/ 995]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 591/ 995]             blk.19.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 592/ 995]             blk.19.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 593/ 995]               blk.19.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 594/ 995]             blk.19.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 595/ 995]             blk.19.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 596/ 995]               blk.19.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 597/ 995]             blk.19.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 598/ 995]             blk.19.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 599/ 995]               blk.19.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 600/ 995]             blk.19.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 601/ 995]             blk.19.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 602/ 995]               blk.19.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 603/ 995]             blk.19.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 604/ 995]             blk.19.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 605/ 995]               blk.19.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 606/ 995]             blk.19.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 607/ 995]             blk.19.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 608/ 995]               blk.19.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 609/ 995]             blk.19.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 610/ 995]             blk.19.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 611/ 995]               blk.19.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 612/ 995]             blk.19.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 613/ 995]             blk.19.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 614/ 995]               blk.19.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 615/ 995]           blk.19.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 616/ 995]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 617/ 995]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 618/ 995]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 619/ 995]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 620/ 995]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 621/ 995]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.018 0.029 0.045 0.064 0.087 0.108 0.245 0.109 0.087 0.064 0.045 0.029 0.018 0.026 \n",
      "[ 622/ 995]             blk.20.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 623/ 995]             blk.20.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 624/ 995]               blk.20.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 625/ 995]             blk.20.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 626/ 995]             blk.20.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 627/ 995]               blk.20.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 628/ 995]             blk.20.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 629/ 995]             blk.20.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 630/ 995]               blk.20.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 631/ 995]           blk.20.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 632/ 995]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 633/ 995]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 634/ 995]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 635/ 995]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.018 0.029 0.045 0.064 0.087 0.109 0.244 0.109 0.087 0.064 0.045 0.029 0.018 0.026 \n",
      "[ 636/ 995]             blk.20.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 637/ 995]             blk.20.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 638/ 995]               blk.20.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 639/ 995]             blk.20.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 640/ 995]             blk.20.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 641/ 995]               blk.20.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 642/ 995]             blk.20.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 643/ 995]             blk.20.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 644/ 995]               blk.20.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 645/ 995]             blk.20.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 646/ 995]             blk.20.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 647/ 995]               blk.20.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 648/ 995]             blk.20.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 649/ 995]             blk.20.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 650/ 995]               blk.20.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 651/ 995]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 652/ 995]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 653/ 995]             blk.21.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 654/ 995]             blk.21.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 655/ 995]               blk.21.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 656/ 995]             blk.21.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 657/ 995]             blk.21.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 658/ 995]               blk.21.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 659/ 995]             blk.21.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 660/ 995]             blk.21.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 661/ 995]               blk.21.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 662/ 995]             blk.21.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 663/ 995]             blk.21.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 664/ 995]               blk.21.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 665/ 995]             blk.21.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 666/ 995]             blk.21.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 667/ 995]               blk.21.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 668/ 995]             blk.21.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 669/ 995]             blk.21.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 670/ 995]               blk.21.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 671/ 995]             blk.21.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 672/ 995]             blk.21.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 673/ 995]               blk.21.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 674/ 995]             blk.21.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 675/ 995]             blk.21.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 676/ 995]               blk.21.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 677/ 995]           blk.21.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 678/ 995]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 679/ 995]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 680/ 995]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 681/ 995]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 682/ 995]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 683/ 995]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.087 0.109 0.246 0.109 0.086 0.064 0.045 0.029 0.018 0.026 \n",
      "[ 684/ 995]             blk.22.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 685/ 995]             blk.22.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 686/ 995]           blk.22.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 687/ 995]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 688/ 995]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 689/ 995]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 690/ 995]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.236 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 691/ 995]               blk.22.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 692/ 995]             blk.22.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 693/ 995]             blk.22.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 694/ 995]               blk.22.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 695/ 995]             blk.22.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 696/ 995]             blk.22.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 697/ 995]               blk.22.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 698/ 995]             blk.22.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 699/ 995]             blk.22.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 700/ 995]               blk.22.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 701/ 995]             blk.22.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 702/ 995]             blk.22.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 703/ 995]               blk.22.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 704/ 995]             blk.22.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 705/ 995]             blk.22.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 706/ 995]               blk.22.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 707/ 995]             blk.22.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 708/ 995]             blk.22.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 709/ 995]               blk.22.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 710/ 995]             blk.22.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 711/ 995]             blk.22.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 712/ 995]               blk.22.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 713/ 995]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 714/ 995]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 715/ 995]             blk.23.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 716/ 995]             blk.23.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 717/ 995]               blk.23.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 718/ 995]             blk.23.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 719/ 995]             blk.23.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 720/ 995]               blk.23.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 721/ 995]             blk.23.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 722/ 995]             blk.23.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 723/ 995]               blk.23.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 724/ 995]             blk.23.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 725/ 995]             blk.23.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 726/ 995]               blk.23.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 727/ 995]             blk.23.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 728/ 995]             blk.23.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 729/ 995]               blk.23.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 730/ 995]             blk.23.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 731/ 995]             blk.23.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 732/ 995]               blk.23.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 733/ 995]             blk.23.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 734/ 995]           blk.23.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 735/ 995]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.235 0.108 0.088 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 736/ 995]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 737/ 995]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 738/ 995]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.239 0.108 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 739/ 995]             blk.23.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 740/ 995]               blk.23.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 741/ 995]             blk.23.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 742/ 995]             blk.23.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 743/ 995]               blk.23.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 744/ 995]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 745/ 995]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 746/ 995]             blk.24.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 747/ 995]             blk.24.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 748/ 995]               blk.24.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 749/ 995]             blk.24.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 750/ 995]             blk.24.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 751/ 995]               blk.24.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 752/ 995]             blk.24.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 753/ 995]             blk.24.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 754/ 995]               blk.24.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 755/ 995]             blk.24.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 756/ 995]             blk.24.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 757/ 995]               blk.24.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 758/ 995]             blk.24.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 759/ 995]             blk.24.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 760/ 995]               blk.24.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 761/ 995]             blk.24.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 762/ 995]             blk.24.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 763/ 995]               blk.24.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 764/ 995]             blk.24.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 765/ 995]             blk.24.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 766/ 995]               blk.24.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 767/ 995]             blk.24.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 768/ 995]             blk.24.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 769/ 995]               blk.24.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 770/ 995]           blk.24.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 771/ 995]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 772/ 995]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 773/ 995]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.088 0.109 0.238 0.109 0.088 0.065 0.045 0.029 0.018 0.026 \n",
      "[ 774/ 995]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 775/ 995]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 776/ 995]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.240 0.107 0.087 0.064 0.046 0.030 0.019 0.027 \n",
      "[ 777/ 995]             blk.25.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 778/ 995]             blk.25.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 779/ 995]               blk.25.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 780/ 995]             blk.25.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 781/ 995]             blk.25.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 782/ 995]               blk.25.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 783/ 995]             blk.25.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 784/ 995]             blk.25.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 785/ 995]               blk.25.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 786/ 995]             blk.25.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 787/ 995]             blk.25.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 788/ 995]               blk.25.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 789/ 995]           blk.25.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 790/ 995]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.065 0.088 0.109 0.238 0.109 0.088 0.065 0.045 0.029 0.018 0.026 \n",
      "[ 791/ 995]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 792/ 995]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 793/ 995]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.064 0.087 0.108 0.243 0.108 0.086 0.064 0.045 0.029 0.019 0.027 \n",
      "[ 794/ 995]             blk.25.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 795/ 995]             blk.25.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 796/ 995]               blk.25.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 797/ 995]             blk.25.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 798/ 995]             blk.25.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 799/ 995]               blk.25.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 800/ 995]             blk.25.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 801/ 995]             blk.25.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 802/ 995]               blk.25.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 803/ 995]             blk.25.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 804/ 995]             blk.25.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 805/ 995]               blk.25.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 806/ 995]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 807/ 995]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 808/ 995]             blk.26.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 809/ 995]             blk.26.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 810/ 995]               blk.26.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 811/ 995]             blk.26.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 812/ 995]             blk.26.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 813/ 995]               blk.26.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 814/ 995]             blk.26.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 815/ 995]             blk.26.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 816/ 995]               blk.26.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 817/ 995]             blk.26.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 818/ 995]             blk.26.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 819/ 995]               blk.26.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 820/ 995]             blk.26.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 821/ 995]             blk.26.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 822/ 995]               blk.26.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 823/ 995]             blk.26.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 824/ 995]             blk.26.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 825/ 995]               blk.26.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 826/ 995]             blk.26.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 827/ 995]             blk.26.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 828/ 995]               blk.26.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 829/ 995]             blk.26.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 830/ 995]             blk.26.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 831/ 995]               blk.26.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 832/ 995]           blk.26.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 833/ 995]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 834/ 995]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 835/ 995]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.065 0.088 0.108 0.237 0.108 0.088 0.065 0.046 0.030 0.018 0.027 \n",
      "[ 836/ 995]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 837/ 995]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 838/ 995]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.045 0.064 0.087 0.108 0.240 0.108 0.087 0.064 0.046 0.030 0.019 0.027 \n",
      "[ 839/ 995]             blk.27.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 840/ 995]             blk.27.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 841/ 995]               blk.27.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 842/ 995]             blk.27.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 843/ 995]             blk.27.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 844/ 995]           blk.27.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 845/ 995]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.236 0.108 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 846/ 995]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 847/ 995]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 848/ 995]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.108 0.235 0.107 0.087 0.065 0.046 0.030 0.019 0.027 \n",
      "[ 849/ 995]               blk.27.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 850/ 995]             blk.27.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 851/ 995]             blk.27.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 852/ 995]               blk.27.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 853/ 995]             blk.27.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 854/ 995]             blk.27.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 855/ 995]               blk.27.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 856/ 995]             blk.27.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 857/ 995]             blk.27.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 858/ 995]               blk.27.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 859/ 995]             blk.27.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 860/ 995]             blk.27.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 861/ 995]               blk.27.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 862/ 995]             blk.27.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 863/ 995]             blk.27.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 864/ 995]               blk.27.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 865/ 995]             blk.27.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 866/ 995]             blk.27.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 867/ 995]               blk.27.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 868/ 995]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 869/ 995]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 870/ 995]             blk.28.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 871/ 995]             blk.28.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 872/ 995]               blk.28.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 873/ 995]             blk.28.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 874/ 995]             blk.28.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 875/ 995]               blk.28.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 876/ 995]             blk.28.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 877/ 995]             blk.28.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 878/ 995]               blk.28.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 879/ 995]             blk.28.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 880/ 995]             blk.28.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 881/ 995]               blk.28.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 882/ 995]             blk.28.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 883/ 995]             blk.28.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 884/ 995]               blk.28.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 885/ 995]             blk.28.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 886/ 995]             blk.28.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 887/ 995]               blk.28.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 888/ 995]             blk.28.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 889/ 995]             blk.28.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 890/ 995]               blk.28.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 891/ 995]             blk.28.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 892/ 995]           blk.28.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 893/ 995]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.028 0.044 0.063 0.087 0.110 0.246 0.110 0.087 0.064 0.044 0.028 0.018 0.026 \n",
      "[ 894/ 995]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 895/ 995]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 896/ 995]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.087 0.109 0.246 0.109 0.087 0.063 0.044 0.029 0.018 0.026 \n",
      "[ 897/ 995]             blk.28.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 898/ 995]               blk.28.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 899/ 995]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 900/ 995]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 901/ 995]             blk.29.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 902/ 995]             blk.29.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 903/ 995]               blk.29.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 904/ 995]             blk.29.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 905/ 995]             blk.29.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 906/ 995]               blk.29.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 907/ 995]             blk.29.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 908/ 995]             blk.29.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 909/ 995]               blk.29.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 910/ 995]             blk.29.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 911/ 995]             blk.29.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 912/ 995]               blk.29.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 913/ 995]             blk.29.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 914/ 995]             blk.29.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 915/ 995]               blk.29.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 916/ 995]             blk.29.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 917/ 995]             blk.29.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 918/ 995]               blk.29.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 919/ 995]             blk.29.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 920/ 995]             blk.29.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 921/ 995]               blk.29.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 922/ 995]             blk.29.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 923/ 995]             blk.29.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 924/ 995]               blk.29.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 925/ 995]           blk.29.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 926/ 995]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 927/ 995]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 928/ 995]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.017 0.027 0.043 0.062 0.086 0.111 0.254 0.112 0.087 0.062 0.043 0.027 0.017 0.026 \n",
      "[ 929/ 995]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 930/ 995]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 931/ 995]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.063 0.085 0.107 0.249 0.108 0.085 0.063 0.045 0.029 0.019 0.027 \n",
      "[ 932/ 995]             blk.30.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 933/ 995]             blk.30.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 934/ 995]               blk.30.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 935/ 995]             blk.30.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 936/ 995]             blk.30.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 937/ 995]               blk.30.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 938/ 995]             blk.30.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 939/ 995]             blk.30.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 940/ 995]               blk.30.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 941/ 995]             blk.30.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 942/ 995]             blk.30.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 943/ 995]               blk.30.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 944/ 995]             blk.30.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 945/ 995]             blk.30.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 946/ 995]               blk.30.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 947/ 995]           blk.30.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 948/ 995]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.017 0.028 0.044 0.063 0.087 0.111 0.246 0.111 0.087 0.063 0.044 0.028 0.018 0.026 \n",
      "[ 949/ 995]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 950/ 995]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 951/ 995]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.017 0.028 0.043 0.062 0.086 0.110 0.257 0.110 0.086 0.062 0.043 0.028 0.017 0.026 \n",
      "[ 952/ 995]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB | hist: \n",
      "[ 953/ 995]             blk.30.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 954/ 995]             blk.30.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 955/ 995]               blk.30.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 956/ 995]             blk.30.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 957/ 995]             blk.30.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 958/ 995]               blk.30.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 959/ 995]             blk.30.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 960/ 995]             blk.30.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 961/ 995]               blk.30.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 962/ 995]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 963/ 995]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 964/ 995]             blk.31.ffn_gate.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 965/ 995]             blk.31.ffn_down.0.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 966/ 995]               blk.31.ffn_up.0.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 967/ 995]             blk.31.ffn_gate.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 968/ 995]             blk.31.ffn_down.1.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 969/ 995]               blk.31.ffn_up.1.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 970/ 995]             blk.31.ffn_gate.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 971/ 995]             blk.31.ffn_down.2.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 972/ 995]               blk.31.ffn_up.2.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 973/ 995]             blk.31.ffn_gate.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 974/ 995]             blk.31.ffn_down.3.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 975/ 995]               blk.31.ffn_up.3.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 976/ 995]             blk.31.ffn_gate.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 977/ 995]             blk.31.ffn_down.4.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 978/ 995]               blk.31.ffn_up.4.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 979/ 995]             blk.31.ffn_gate.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 980/ 995]             blk.31.ffn_down.5.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 981/ 995]               blk.31.ffn_up.5.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 982/ 995]             blk.31.ffn_gate.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 983/ 995]             blk.31.ffn_down.6.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 984/ 995]               blk.31.ffn_up.6.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 985/ 995]             blk.31.ffn_gate.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 986/ 995]             blk.31.ffn_down.7.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 987/ 995]               blk.31.ffn_up.7.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
      "[ 988/ 995]           blk.31.ffn_gate_inp.weight - [ 4096,     8,     1,     1], type =    f16, size =    0.062 MB\n",
      "[ 989/ 995]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 990/ 995]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 991/ 995]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.087 0.110 0.242 0.110 0.087 0.064 0.045 0.029 0.018 0.026 \n",
      "[ 992/ 995]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 993/ 995]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
      "[ 994/ 995]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q8_0 .. size =     8.00 MiB ->     4.25 MiB | hist: 0.000 0.026 0.018 0.028 0.044 0.063 0.086 0.110 0.251 0.110 0.086 0.063 0.044 0.028 0.018 0.026 \n",
      "[ 995/ 995]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 89079.02 MB\n",
      "llama_model_quantize_internal: quant size  = 25215.87 MB\n",
      "llama_model_quantize_internal: hist: 0.000 0.026 0.018 0.029 0.044 0.063 0.085 0.107 0.255 0.107 0.085 0.063 0.044 0.029 0.018 0.026 \n",
      "\n",
      "main: quantize time = 1420340.70 ms\n",
      "main:    total time = 1420340.70 ms\n"
     ]
    }
   ],
   "source": [
    "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
    "for method in [\"q4_k_m\"]:\n",
    "    qtype = f\"/home/ubuntu/Mixtral-8x7B-Instruct-v0.1-GGUF/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
    "    !/home/ubuntu/llama.cpp/quantize {fp16} {qtype} {method}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
